{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T12:53:31.267233Z",
     "start_time": "2020-05-24T12:53:31.261907Z"
    }
   },
   "source": [
    "# Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T09:00:28.226510Z",
     "start_time": "2020-05-24T09:00:25.488404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/mostafa/.local/lib/python3.6/site-packages (3.3)\n",
      "Requirement already satisfied: six in /home/mostafa/.local/lib/python3.6/site-packages (from nltk) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T12:56:26.505397Z",
     "start_time": "2020-05-24T12:55:13.807565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', 'stopwords', 'punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:51:57.747858Z",
     "start_time": "2020-05-24T15:51:57.739762Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import sqrt, log\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from math import log\n",
    "from nltk.corpus import stopwords as sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T18:08:37.624117Z",
     "start_time": "2020-05-20T18:08:37.604856Z"
    }
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:48:32.113665Z",
     "start_time": "2020-05-24T15:48:32.108507Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(path):    \n",
    "    '''Reading the json files from passed path'''\n",
    "    with open(path) as d:\n",
    "        json_data = json.load(d)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T12:50:56.287616Z",
     "start_time": "2020-05-24T12:50:56.276468Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_terms(doc_info):\n",
    "    '''concatenate all terms (title + body) of the passed doc'''\n",
    "    body_terms = doc_info['body'].split()\n",
    "    title_terms = doc_info['title'].split()\n",
    "    all_terms = body_terms + title_terms\n",
    "    return all_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count term frequency over collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:52:02.883196Z",
     "start_time": "2020-05-24T15:52:02.878260Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = sw.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:52:07.555744Z",
     "start_time": "2020-05-24T15:52:07.531938Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_document_frequency(json_data):\n",
    "    vectors = {}\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for doc_id, doc in enumerate(json_data):\n",
    "        terms = get_doc_terms(doc)\n",
    "        for term in terms:\n",
    "            if term not in stopwords:\n",
    "                term = lemmatizer.lemmatize(term)\n",
    "                if term not in vectors:\n",
    "                    vectors[term] = []\n",
    "                if doc_id not in vectors[term]:\n",
    "                    vectors[term].append(doc_id)\n",
    "    \n",
    "    vectors_list = list(vectors.items())\n",
    "    term_map_index = {}\n",
    "    \n",
    "    for i in range(len(vectors_list)):\n",
    "        term = vectors_list[i][0]\n",
    "        term_map_index[term] = i\n",
    "    \n",
    "    frequencies = []\n",
    "    for word, freq_list in vectors_list:\n",
    "        frequencies.append(len(freq_list))\n",
    "    \n",
    "    return term_map_index, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T13:11:03.267467Z",
     "start_time": "2020-05-24T13:11:03.244143Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vector(json_data, term_map_index, frequencies, train_docs_no = 24000):\n",
    "    term_no = len(term_map_index)\n",
    "    vector = np.zeros((train_docs_no, term_no))\n",
    "        \n",
    "    for doc_id, doc in enumerate(json_data):\n",
    "        terms = get_doc_terms(doc)\n",
    "        doc_term_freq = {}\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in stopwords:\n",
    "                if term not in doc_term_freq:\n",
    "                    doc_term_freq[term] = 0\n",
    "                doc_term_freq[term] += 1\n",
    "                \n",
    "        for term in doc_term_freq:\n",
    "            if term in term_map_index:\n",
    "                index = term_map_index[term]\n",
    "                doc_frequency = frequencies[index]\n",
    "                tf = doc_term_freq[term]\n",
    "                idf = log(train_docs_no/doc_frequency)\n",
    "                vector[doc_id, index] = tf * idf\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(data):\n",
    "    '''return list of categories for train and test docs'''\n",
    "    categories = []\n",
    "    \n",
    "    for doc in data:\n",
    "        category = doc['category']\n",
    "        categories.append(category)\n",
    "        \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:59:02.932185Z",
     "start_time": "2020-05-24T15:58:56.012131Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = get_data('./data/train.json')\n",
    "test_data = get_data('./data/validation.json')\n",
    "# print(list(train_data)[:10])\n",
    "\n",
    "train_categories = get_category(train_data)\n",
    "test_categories = get_category(test_data)\n",
    "# print(train_categories)[:10]\n",
    "\n",
    "N = len(train_data)\n",
    "term_map_index, frequencies = get_document_frequency(train_data)\n",
    "\n",
    "N = N // 10\n",
    "\n",
    "#randomly pick N data from train\n",
    "train_data = train_data[:N]\n",
    "test_data = test_data[:300]\n",
    "\n",
    "#train and test vectors \n",
    "train_vector = get_vector(train_data, term_map_index, frequencies, N)\n",
    "test_vector = get_vector(test_data, term_map_index, frequencies, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN (k Nearest Neighbor)\n",
    "\n",
    "There are two methods used for measuring nearest neighbors: \n",
    "1. Cosine Similarity\n",
    "2. Euclidean Distance\n",
    "\n",
    "The Following sections would be the implementation of each method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine Similarity = train.test^t / |train|.|test^t|\n",
    "\n",
    "Since |test^t| is constant, we can ignore its value and don't calculate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T14:27:21.068669Z",
     "start_time": "2020-05-24T14:27:21.064739Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cosine_similarity(train, test):\n",
    "    '''consine_similarity = train.test^t / |train|.|test|'''\n",
    "    test_transpose = test.T\n",
    "    dot_product = np.dot(train, test_transpose)\n",
    "    normalization = np.linalg.norm(train, axis=1)\n",
    "    similarity = dot_product / normalization\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:24:04.741933Z",
     "start_time": "2020-05-24T15:24:04.732423Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_euclidean_distance(train, test):\n",
    "    '''Euclidean Distance'''\n",
    "    x_2 = np.sum(test * test, axis=1)\n",
    "    test_transpose = test.T\n",
    "    xy = np.dot(train, test_transpose, axis=1)\n",
    "    y_2 = np.sum(train * train, axis = 1)\n",
    "    \n",
    "    #make the test 1-column for evaluation\n",
    "    x_2 = np.reshape(x_2, shape(-1, 1))\n",
    "    result = x_2 + y_2 - 2 * xy\n",
    "    euclidean_distance = np.sqrt(result)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(doc_id, scores, category, method, k):\n",
    "    \n",
    "    \n",
    "valid_k = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T11:51:05.892180Z",
     "start_time": "2020-05-16T11:51:05.884485Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T12:11:05.669404Z",
     "start_time": "2020-05-18T12:11:05.111375Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_category_count(category_no = 4):\n",
    "    category_count = {}\n",
    "    \n",
    "    for i in range(1, category_no + 1):\n",
    "        category_count[str(i)] = 0\n",
    "    \n",
    "    for doc_id in vectors:\n",
    "        category_id = str(vectors[doc_id]['category'])\n",
    "        category_count[category_id] += 1\n",
    "    \n",
    "    return category_count\n",
    "\n",
    "def naive_bayes_training(category_no=4):\n",
    "    N = len(json_data)\n",
    "    category_count = get_category_count()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

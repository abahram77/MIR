{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T12:53:31.267233Z",
     "start_time": "2020-05-24T12:53:31.261907Z"
    }
   },
   "source": [
    "# Installing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:43:31.120883Z",
     "start_time": "2020-05-26T19:43:25.953838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
      "Requirement already satisfied: click in /home/mostafa/.local/lib/python3.6/site-packages (from nltk) (7.1.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.46.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2020.5.14)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.15.1)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/mostafa/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/mostafa/.local/lib/python3.6/site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/mostafa/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mostafa/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (2.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/mostafa/.local/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/mostafa/.local/lib/python3.6/site-packages (1.18.2)\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/mostafa/.local/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.6.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mostafa/.local/lib/python3.6/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/mostafa/.local/lib/python3.6/site-packages (from matplotlib) (1.18.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/mostafa/.local/lib/python3.6/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/mostafa/.local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in /home/mostafa/.local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install sklearn\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T13:20:19.869750Z",
     "start_time": "2020-05-25T13:19:35.249262Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet', 'stopwords', 'punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:44:03.432943Z",
     "start_time": "2020-05-26T19:44:03.421967Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from math import sqrt, log\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from math import log\n",
    "from nltk.corpus import stopwords as sw\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:44:05.302335Z",
     "start_time": "2020-05-26T19:44:05.298795Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T18:08:37.624117Z",
     "start_time": "2020-05-20T18:08:37.604856Z"
    }
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:15.135333Z",
     "start_time": "2020-05-26T20:10:15.129055Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(path):    \n",
    "    '''Reading the json files from passed path'''\n",
    "    with open(path) as d:\n",
    "        json_data = json.load(d)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:16.094648Z",
     "start_time": "2020-05-26T20:10:16.075626Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_terms(doc_info):\n",
    "    '''concatenate all terms (title + body) of the passed doc'''\n",
    "    body_terms = doc_info['body'].split()\n",
    "    title_terms = doc_info['title'].split()\n",
    "    all_terms = body_terms + title_terms\n",
    "    return all_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count term frequency over collection\n",
    "\n",
    "### get_document_frequency:\n",
    "\n",
    "Counts each term frequency and assign an index to the term. \n",
    "\n",
    "map_to_index structure is like the following: {term: index}\n",
    "\n",
    "frequencies returns the frequency of each term: {term: frequency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:16.739420Z",
     "start_time": "2020-05-26T20:10:16.724520Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = sw.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:17.013163Z",
     "start_time": "2020-05-26T20:10:16.975621Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_document_frequency(json_data, stemming=False, lemmatizing=False):\n",
    "    vectors = {}\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for doc_id, doc in enumerate(json_data):\n",
    "        terms = get_doc_terms(doc)\n",
    "        for term in terms:\n",
    "            if term not in stopwords:\n",
    "                if lemmatizing:\n",
    "                    term = lemmatizer.lemmatize(term)\n",
    "                if stemming:\n",
    "                    term = stemmer.stem(term)\n",
    "                if term not in vectors:\n",
    "                    vectors[term] = []\n",
    "                if doc_id not in vectors[term]:\n",
    "                    vectors[term].append(doc_id)\n",
    "    \n",
    "    vectors_list = list(vectors.items())\n",
    "    term_map_index = {}\n",
    "    \n",
    "    for i in range(len(vectors_list)):\n",
    "        term = vectors_list[i][0]\n",
    "        term_map_index[term] = i\n",
    "    \n",
    "    frequencies = []\n",
    "    for word, freq_list in vectors_list:\n",
    "        frequencies.append(len(freq_list))\n",
    "    \n",
    "    return term_map_index, frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  get_vector:\n",
    "\n",
    "creates a matrix with two axis: \n",
    "\n",
    "1. category \n",
    "2. term\n",
    "\n",
    "each element represents the idf * tf of term in the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:17.513924Z",
     "start_time": "2020-05-26T20:10:17.465334Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vector(json_data, term_map_index, frequencies, train_docs_no, stemming=False, lemmatizing=False):\n",
    "    term_no = len(term_map_index)\n",
    "    data_no = len(json_data)\n",
    "    vector = np.zeros((data_no, term_no))\n",
    "        \n",
    "    for doc_id, doc in enumerate(json_data):\n",
    "        terms = get_doc_terms(doc)\n",
    "        doc_term_freq = {}\n",
    "        \n",
    "        for term in terms:\n",
    "            if term not in stopwords:\n",
    "                if lemmatizing:\n",
    "                    term = lemmatizer.lemmatize(term)\n",
    "                if stemming:\n",
    "                    term = stemmer.stem(term)\n",
    "                if term not in doc_term_freq:\n",
    "                    doc_term_freq[term] = 0\n",
    "                doc_term_freq[term] += 1\n",
    "                \n",
    "        for term in doc_term_freq:\n",
    "            if term in term_map_index:\n",
    "                index = term_map_index[term]\n",
    "                doc_frequency = frequencies[index]\n",
    "                tf = doc_term_freq[term]\n",
    "                idf = log(train_docs_no/doc_frequency)\n",
    "                vector[doc_id, index] = tf * idf\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_category\n",
    "\n",
    "returns sequential category of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:18.135298Z",
     "start_time": "2020-05-26T20:10:18.114012Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_category(data):\n",
    "    '''return list of categories for train and test docs'''\n",
    "    categories = []\n",
    "    \n",
    "    for doc in data:\n",
    "        category = doc['category']\n",
    "        categories.append(category)\n",
    "        \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning variables\n",
    "\n",
    "Since the training data size is very large (about 13 GB of memory is needed), it has been reduced to the tweleveth. \n",
    "\n",
    "The same scenario is for the test data size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:19.387062Z",
     "start_time": "2020-05-26T20:10:18.666754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.6932623386383057\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_data = get_data('./data/train.json')\n",
    "test_data = get_data('./data/validation.json')\n",
    "# print(list(train_data)[:10])\n",
    "\n",
    "\n",
    "N = len(train_data)\n",
    "\n",
    "N = N // 12\n",
    "\n",
    "#randomly pick N data from train\n",
    "train_data = train_data[:N]\n",
    "test_data = test_data[:300]\n",
    "\n",
    "term_map_index, frequencies = get_document_frequency(train_data, lemmatizing=True)\n",
    "\n",
    "train_categories = get_category(train_data)\n",
    "test_categories = get_category(test_data)\n",
    "# print(train_categories)[:10]\n",
    "\n",
    "#train and test vectors \n",
    "train_vector = get_vector(train_data, term_map_index, frequencies, N)\n",
    "test_vector = get_vector(test_data, term_map_index, frequencies, N)\n",
    "\n",
    "finish_time = time.time()\n",
    "print('elapsed time:', finish_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing vs stemming:\n",
    "\n",
    "As the result shows, the lemmatizing works better than stemming on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN (k Nearest Neighbor)\n",
    "\n",
    "There are two methods used for measuring nearest neighbors: \n",
    "1. Cosine Similarity\n",
    "2. Euclidean Distance\n",
    "\n",
    "The Following sections would be the implementation of each method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "Cosine Similarity = train.test^t / |train|.|test^t|\n",
    "\n",
    "Since |test^t| is constant, we can ignore its value and don't calculate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:20.364174Z",
     "start_time": "2020-05-26T20:10:20.340250Z"
    }
   },
   "outputs": [],
   "source": [
    "euclidean_distance = None\n",
    "def calculate_euclidean_distance(train, test):\n",
    "    '''Euclidean Distance'''\n",
    "    global euclidean_distance\n",
    "    x_2 = np.sum(test * test, axis=1)\n",
    "    test_transpose = test.T\n",
    "    xy = np.dot(train, test_transpose).T\n",
    "    y_2 = np.sum(train * train, axis = 1)\n",
    "    \n",
    "    #make the test 1-column for evaluation\n",
    "    x_2 = x_2[:,None]\n",
    "    result = x_2 + y_2 - 2 * xy\n",
    "    euclidean_distance = np.sqrt(result)\n",
    "    return euclidean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:20.829125Z",
     "start_time": "2020-05-26T20:10:20.815411Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity = None\n",
    "def calculate_cosine_similarity(train, test):\n",
    "    '''consine_similarity = train.test^t / |train|.|test|'''\n",
    "    global cosine_similarity\n",
    "    test_transpose = test.T\n",
    "    dot_product = np.dot(train, test_transpose).T\n",
    "    normalization = np.linalg.norm(train, axis=1)\n",
    "    cosine_similarity = dot_product / normalization\n",
    "    return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN \n",
    "\n",
    "As the k becomes larger (in this case 5 has the best result among 1 and 3), the algorithm performs better. That sounds reasonable, because of more neighbors are checked to determine the class/category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:21.647729Z",
     "start_time": "2020-05-26T20:10:21.627979Z"
    }
   },
   "outputs": [],
   "source": [
    "def knn(doc_id, method, k=5):\n",
    "    global cosine_similarity\n",
    "    global euclidean_distance\n",
    "    if method == 'cosine':\n",
    "        similarities = cosine_similarity[doc_id]\n",
    "        nearest = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:k]\n",
    "    elif method == 'euclidean':\n",
    "        distances = euclidean_distance[doc_id]\n",
    "        nearest = sorted(range(len(distances)), key=lambda i: distances[i])[:k]\n",
    "    categories = {}\n",
    "    for i in range(len(nearest)):\n",
    "        category = train_categories[nearest[i]]\n",
    "        if category not in categories:\n",
    "          categories[category] = 0\n",
    "        categories[category] += 1\n",
    "    \n",
    "    return max(categories, key=categories.get)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-16T11:51:05.892180Z",
     "start_time": "2020-05-16T11:51:05.884485Z"
    }
   },
   "source": [
    "# Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:22.478707Z",
     "start_time": "2020-05-26T20:10:22.463227Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vector_creation_time(method):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if method == 'euclidean':\n",
    "        calculate_euclidean_distance(train_vector, test_vector)\n",
    "    elif method == 'cosine':\n",
    "        calculate_cosine_similarity(train_vector, test_vector)\n",
    "    finish_time = time.time()\n",
    "    elapsed_time = finish_time - start_time\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:22.862436Z",
     "start_time": "2020-05-26T20:10:22.823405Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_recall(method, category_no=4, k=5):\n",
    "    tp, total = [], []\n",
    "    \n",
    "    for i in range(category_no):\n",
    "        tp.append(0)\n",
    "        total.append(0)\n",
    "        \n",
    "    test_no = len(test_data)\n",
    "    for i in range(test_no):\n",
    "        real = test_data[i]['category']\n",
    "        if method == 'bayes':\n",
    "          predicted = naive_bayes_testing(test_data[i], 1)\n",
    "        else:\n",
    "          predicted = knn(i, method, k)\n",
    "        index = real - 1\n",
    "        \n",
    "        if real == predicted:\n",
    "            tp[index] += 1\n",
    "            \n",
    "        total[index] += 1\n",
    "    \n",
    "    recall = {}\n",
    "    \n",
    "    for i in range(category_no):\n",
    "        index = i + 1\n",
    "        recall[index] = tp[i]/total[i]\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:23.172530Z",
     "start_time": "2020-05-26T20:10:23.126191Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_precision(method, category_no=4, k=5):\n",
    "    tp, total = [], []\n",
    "    \n",
    "    for i in range(category_no):\n",
    "        tp.append(0)\n",
    "        total.append(0)\n",
    "        \n",
    "    test_no = len(test_data)\n",
    "    for i in range(test_no):\n",
    "        real = test_data[i]['category']\n",
    "        if method == 'bayes':\n",
    "          predicted = naive_bayes_testing(test_data[i], 1)\n",
    "        else:\n",
    "          predicted = knn(i, method, k)\n",
    "        index = predicted - 1\n",
    "        \n",
    "        if real == predicted:\n",
    "            tp[real - 1] += 1\n",
    "            \n",
    "        total[index] += 1\n",
    "    \n",
    "    precision = {}\n",
    "    \n",
    "    for i in range(category_no):\n",
    "        index = i + 1\n",
    "        precision[index] = tp[i]/total[i]\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:23.478860Z",
     "start_time": "2020-05-26T20:10:23.464098Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_accuracy(method, category_no=4, k=5):\n",
    "    test_no = len(test_data)\n",
    "    tp = 0\n",
    "    \n",
    "    for i in range(test_no):\n",
    "        if method == 'bayes':\n",
    "          predicted = naive_bayes_testing(test_data[i], 1)\n",
    "        else:\n",
    "          predicted = knn(i, method, k)\n",
    "        real = test_data[i]['category']\n",
    "        \n",
    "        if predicted == real:\n",
    "            tp += 1\n",
    "    \n",
    "    accuracy = tp/test_no\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:23.862414Z",
     "start_time": "2020-05-26T20:10:23.843251Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_marco_f1(precision_val, recall_val, category_no=4):\n",
    "    results = []\n",
    "    for i in range(1, category_no+1):\n",
    "        result = 2 * precision_val[i] * recall_val[i] / (precision_val[i] + recall_val[i])\n",
    "        results.append(result)\n",
    "    return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:25.584138Z",
     "start_time": "2020-05-26T20:10:24.826254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7487761974334717"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector_creation_time('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:25.927418Z",
     "start_time": "2020-05-26T20:10:25.586200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.36046511627906974,\n",
       " 2: 0.04054054054054054,\n",
       " 3: 0.9382716049382716,\n",
       " 4: 0.2542372881355932}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_recall = get_recall('euclidean')\n",
    "euclidean_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:27.492974Z",
     "start_time": "2020-05-26T20:10:27.091527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7560975609756098, 2: 1.0, 3: 0.3247863247863248, 4: 0.6818181818181818}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_precision = get_precision('euclidean')\n",
    "euclidean_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:28.040471Z",
     "start_time": "2020-05-26T20:10:27.619087Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4166666666666667"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:28.047447Z",
     "start_time": "2020-05-26T20:10:28.042957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35475527680252095"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_marco_f1(euclidean_precision, euclidean_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:28.881980Z",
     "start_time": "2020-05-26T20:10:28.110036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7680037021636963"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector_creation_time('cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:29.141260Z",
     "start_time": "2020-05-26T20:10:28.884797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.8023255813953488,\n",
       " 2: 0.8918918918918919,\n",
       " 3: 0.7530864197530864,\n",
       " 4: 0.7796610169491526}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_recall = get_recall('cosine')\n",
    "cosine_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:29.500590Z",
     "start_time": "2020-05-26T20:10:29.142851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.8846153846153846,\n",
       " 2: 0.868421052631579,\n",
       " 3: 0.782051282051282,\n",
       " 4: 0.6764705882352942}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_precision = get_precision('cosine')\n",
    "cosine_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:29.849673Z",
     "start_time": "2020-05-26T20:10:29.502134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8066666666666666"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy('cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:30.419990Z",
     "start_time": "2020-05-26T20:10:30.406173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8032921152343301"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_marco_f1(cosine_precision, cosine_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing naive bayes with a constant value of alpha, the best result was for 0.368 for 10 randomly generated values between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:31.548331Z",
     "start_time": "2020-05-26T20:10:31.521941Z"
    }
   },
   "outputs": [],
   "source": [
    "n_c, t_c = None, None\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def naive_bayes_training(data, category_no=4):\n",
    "    global n_c\n",
    "    global t_c\n",
    "    term_no = len(term_map_index)\n",
    "    \n",
    "    n_c = []\n",
    "    for i in range(category_no):\n",
    "        n_c.append(0)\n",
    "    \n",
    "    t_c = np.zeros((category_no, term_no))\n",
    "    \n",
    "    for doc_id, doc in enumerate(data):\n",
    "        terms = get_doc_terms(doc)\n",
    "        category = doc['category']\n",
    "\n",
    "        for term in terms:\n",
    "            index = category - 1\n",
    "            if term not in stopwords:\n",
    "                term = lemmatizer.lemmatize(term)\n",
    "                t_c[index, term_map_index[term]] += 1\n",
    "            n_c[index] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:14:05.168775Z",
     "start_time": "2020-05-26T20:14:05.149936Z"
    }
   },
   "outputs": [],
   "source": [
    "def naive_bayes_testing(document, alpha=0.368, category_no=4):\n",
    "    global n_c\n",
    "    global t_c\n",
    "    term_no = len(term_map_index)\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(category_no):\n",
    "        score = log(n_c[i]/N)\n",
    "        scores.append(score)\n",
    "    \n",
    "    sigma = np.sum(t_c, axis=1)\n",
    "    \n",
    "    for i in range(category_no):\n",
    "        terms = get_doc_terms(document)\n",
    "        for term in terms:\n",
    "            if term not in stopwords:\n",
    "                term = lemmatizer.lemmatize(term)\n",
    "                if term in term_map_index:\n",
    "                    fraction = log((t_c[i, term_map_index[term]] + alpha) / (sigma[i] + term_no))\n",
    "                    scores[i] += fraction\n",
    "    predicted = np.argmax(scores) + 1\n",
    "\n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:32.965997Z",
     "start_time": "2020-05-26T20:10:32.501651Z"
    }
   },
   "outputs": [],
   "source": [
    "naive_bayes_training(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:33.220340Z",
     "start_time": "2020-05-26T20:10:32.967515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.9069767441860465,\n",
       " 2: 0.9459459459459459,\n",
       " 3: 0.8641975308641975,\n",
       " 4: 0.864406779661017}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_recall = get_recall('bayes')\n",
    "bayes_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:33.696111Z",
     "start_time": "2020-05-26T20:10:33.342484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.9512195121951219,\n",
       " 2: 0.9210526315789473,\n",
       " 3: 0.9090909090909091,\n",
       " 4: 0.7846153846153846}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_precision = get_precision('bayes')\n",
    "bayes_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:34.207505Z",
     "start_time": "2020-05-26T20:10:33.860224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8966666666666666"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy('bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:10:34.214702Z",
     "start_time": "2020-05-26T20:10:34.209764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926403391082852"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_marco_f1(bayes_precision, bayes_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Changing the default value of C (1), doesn't show noticable change in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:12:58.379221Z",
     "start_time": "2020-05-26T20:10:34.588390Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "clf.fit(train_vector, train_categories)\n",
    "finish_time = time.time()\n",
    "elapsed_time = finish_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:12:58.387618Z",
     "start_time": "2020-05-26T20:12:58.380818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.78522396087646"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:13:13.025165Z",
     "start_time": "2020-05-26T20:12:58.390718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8433333333333334"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_vector, test_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:13:14.898104Z",
     "start_time": "2020-05-26T20:13:13.026863Z"
    }
   },
   "outputs": [],
   "source": [
    "start_RandomForestClassifiermForestClassifiermax_depth=Classifier= time.time()\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "clf.fit(train_vector, train_categories)\n",
    "finish_time = time.time()\n",
    "elapsed_time = finish_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T20:13:14.927397Z",
     "start_time": "2020-05-26T20:13:14.899926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6733333333333333"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_vector, test_categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

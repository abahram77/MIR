{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_stopwords(directory):\n",
    "    stopwords = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            for stopword in f.read().splitlines():\n",
    "                if stopword not in stopwords:\n",
    "                    stopwords.append(stopword)\n",
    "    return stopwords        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = []\n",
    "    normalizer = Normalizer()\n",
    "    normalized_text = normalizer.normalize(raw_text)\n",
    "    tokenized_text = word_tokenize(normalized_text)\n",
    "    stopwords = get_stopwords('stopwords')\n",
    "    stemmer = Stemmer()\n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        if token not in stopwords:\n",
    "            stemmed_text = stemmer.stem(token)\n",
    "            if stemmed_text != \"\":\n",
    "                prepared_text.append(stemmed_text)\n",
    "\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = []\n",
    "    stopwords = get_stopwords('stopwords')\n",
    "    \n",
    "    normalizer = Normalizer()\n",
    "    normalized_text = normalizer.normalize(raw_text)\n",
    "    for stop in stopwords:\n",
    "        normalized_text = normalized_text.replace(stop, \" \")\n",
    "    tokenized_text = word_tokenize(normalized_text)\n",
    "    stemmer = Stemmer()\n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        if token not in stopwords:\n",
    "            stemmed_text = stemmer.stem(token)\n",
    "            if stemmed_text != \"\":\n",
    "                prepared_text.append(stemmed_text)\n",
    "\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "from xml.dom.minidom import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "# Reading Data (XML file) and store it in dom variable\n",
    "dom = parse('project1_data/data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_title(raw_title):\n",
    "    title = raw_title[0].childNodes[0].data\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_text(raw_text):\n",
    "    text = raw_text[0].childNodes[0].data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_id(raw_id):\n",
    "    doc_id = raw_id[0].childNodes[0].data\n",
    "    return doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "positional_index = {}\n",
    "doc_length = {} #number of words for each documents\n",
    "\n",
    "def document_length(title_words_list, text_words_list, doc_id):\n",
    "    all_words = []\n",
    "    for word in title_words_list:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    for word in text_words_list:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "    doc_length[doc_id] = len(all_words)\n",
    "    \n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    dom = parse(docs_path)\n",
    "    pages = dom.getElementsByTagName('page')    \n",
    "    for page in pages:\n",
    "        title = get_title(page.getElementsByTagName('title'))\n",
    "        text = get_text(page.getElementsByTagName('text'))\n",
    "        doc_id = get_id(page.getElementsByTagName('id'))\n",
    "        \n",
    "        text_words_list = prepare_text(text)\n",
    "        title_words_list = prepare_text(title)\n",
    "        \n",
    "        document_length(title_words_list, text_words_list, doc_id)\n",
    "        \n",
    "        for index, text_word in enumerate(text_words_list):\n",
    "            if text_word not in positional_index:\n",
    "                initializer = {doc_id: {'text': []}}\n",
    "                positional_index[text_word] = initializer\n",
    "            elif doc_id not in positional_index[text_word]:\n",
    "                positional_index[text_word][doc_id] = {'text': []}\n",
    "            elif 'text' not in positional_index[text_word][doc_id]:\n",
    "                positional_index[text_word][doc_id]['text'] = []\n",
    "            positional_index[text_word][doc_id]['text'].append(index)\n",
    "\n",
    "            \n",
    "        for index, title_word in enumerate(title_words_list):\n",
    "            if title_word not in positional_index:\n",
    "                initializer = {doc_id: {'title': []}}\n",
    "                positional_index[title_word] = initializer\n",
    "            elif doc_id not in positional_index[title_word]:\n",
    "                positional_index[title_word][doc_id] = {'title': []}\n",
    "            elif 'title' not in positional_index[title_word][doc_id]:\n",
    "                positional_index[title_word][doc_id]['title'] = []\n",
    "            positional_index[title_word][doc_id]['title'].append(index)\n",
    "    \n",
    "\n",
    "construct_positional_indexes('project1_data/data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_posting_list(word):\n",
    "    # Make sure how to get posting list! From reading file or as an argument\n",
    "    return positional_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['میانگین\\u200cبارش\\u200cسالانه',\n",
       " 'تلاق',\n",
       " 'ساوجبلاغ',\n",
       " 'سابلاغ',\n",
       " 'ولا',\n",
       " 'اعلا',\n",
       " 'اسقلال',\n",
       " 'میلاد',\n",
       " 'دلال',\n",
       " 'لاز',\n",
       " 'خلاصه',\n",
       " 'علاقه',\n",
       " 'قبلا',\n",
       " 'علاوه',\n",
       " 'اطلاعات',\n",
       " 'اطلاع',\n",
       " 'علام',\n",
       " 'بالا',\n",
       " 'طولانی\\u200cمد',\n",
       " 'علاقه\\u200cمند',\n",
       " 'اولاد',\n",
       " 'استقلال',\n",
       " 'اسلام',\n",
       " 'هلال',\n",
       " 'انقلاب',\n",
       " 'ولایة',\n",
       " 'سنگلاخ',\n",
       " 'الان',\n",
       " 'اسلا',\n",
       " 'الاستعمار',\n",
       " 'خلاف',\n",
       " 'الاستعماریه',\n",
       " 'سالانه',\n",
       " 'الانوار',\n",
       " 'سطح\\u200cبالا',\n",
       " 'مردم\\u200cسالار',\n",
       " 'بالاترین',\n",
       " 'کاملا',\n",
       " 'الاسلا',\n",
       " 'اسلام\\u200cگرا',\n",
       " 'معمولا',\n",
       " 'لاکرب',\n",
       " 'الاخضر',\n",
       " 'جبل\\u200cالاخضر',\n",
       " 'مسلاته',\n",
       " 'دلار',\n",
       " 'کالا',\n",
       " 'مبتلا',\n",
       " 'لاتین',\n",
       " 'اسلاو',\n",
       " 'مالاگا',\n",
       " 'لاذقیه',\n",
       " 'اللاذقیة',\n",
       " 'للمیلاد',\n",
       " 'والاسلامیة',\n",
       " 'ایلا',\n",
       " 'اردلان',\n",
       " 'ایلام',\n",
       " 'اصطلاح',\n",
       " 'سلاطین',\n",
       " 'اطلاق',\n",
       " 'مثلا',\n",
       " 'الاکراد',\n",
       " 'الارض',\n",
       " 'الانبیا',\n",
       " 'الانبیاء',\n",
       " 'الصلاه',\n",
       " 'السلا',\n",
       " 'ولادمیر',\n",
       " 'ملاطیه',\n",
       " 'احتمالا',\n",
       " 'ملا',\n",
       " 'تلا',\n",
       " 'تازه\\u200cاستقلال\\u200cیافته',\n",
       " 'اختلاف',\n",
       " 'غلامرضا',\n",
       " 'باجلانی',\n",
       " 'ولادیمیر',\n",
       " 'اطلاعات\\u200cنامه',\n",
       " 'طولان',\n",
       " 'ملاحظه',\n",
       " 'دالاهو',\n",
       " 'اختلاط',\n",
       " 'اخلاف',\n",
       " 'هه\\u200cلاله',\n",
       " 'هلاله',\n",
       " 'دلاور',\n",
       " 'سالار',\n",
       " 'لایق',\n",
       " 'جلال',\n",
       " 'پلاک',\n",
       " 'سونگورلاره',\n",
       " 'زلاندنو',\n",
       " 'لارنس',\n",
       " 'خلال',\n",
       " 'زلاند',\n",
       " 'ملاق',\n",
       " 'سولاندر',\n",
       " 'بلافاصله',\n",
       " 'عملا',\n",
       " 'تلاش',\n",
       " 'کیالاکه\\u200cکوا',\n",
       " 'دلایل',\n",
       " 'آلاسکا',\n",
       " 'سلاح',\n",
       " 'کلاسیک',\n",
       " 'اصلاح',\n",
       " 'بعلاوه',\n",
       " 'ایرلاینز',\n",
       " 'بلاویا',\n",
       " 'فلای\\u200cدب',\n",
       " 'لارناکا',\n",
       " 'بلامانع',\n",
       " 'سال\\u200cولایت\\u200cشدن',\n",
       " 'علاقه\\u200cداری',\n",
       " 'ولایت',\n",
       " 'مواصلات',\n",
       " 'بالامرغاب',\n",
       " 'الایا',\n",
       " 'فلادلفیا',\n",
       " 'ولایات',\n",
       " 'اصلا',\n",
       " 'الاخبار',\n",
       " 'حالا',\n",
       " 'بالاخره',\n",
       " 'مشهورولا',\n",
       " 'فعلا',\n",
       " 'مولانا',\n",
       " 'غلام\\u200cحضر',\n",
       " 'پلانگذار',\n",
       " 'آلات',\n",
       " 'سلام',\n",
       " 'عبدالاحمد',\n",
       " 'مرکزولا',\n",
       " 'سکولار',\n",
       " 'لاهیج',\n",
       " 'طولانی',\n",
       " 'میلان',\n",
       " 'دیوانسالار',\n",
       " 'اصلاح\\u200cاند',\n",
       " 'علامه',\n",
       " 'اصلاح\\u200cطلب',\n",
       " 'انقلابیون',\n",
       " 'کلاس',\n",
       " 'استدلال',\n",
       " 'برخلاف',\n",
       " 'طلاب',\n",
       " 'اعلامیه',\n",
       " 'اصلاح\\u200cنشده',\n",
       " 'اسلامگرا',\n",
       " 'خلاص',\n",
       " 'انحلال',\n",
       " 'بی\\u200cملاحظه',\n",
       " 'اصلاحگر',\n",
       " 'ارباب\\u200cسالار',\n",
       " 'گلایه',\n",
       " 'اطلاع\\u200cرسان',\n",
       " 'کلانتری',\n",
       " 'پالایشگاه',\n",
       " 'ائتلاف',\n",
       " 'لاهوت',\n",
       " 'ابلاغ',\n",
       " 'تشکیلات',\n",
       " 'ممنوع\\u200cالانتشار',\n",
       " 'اختلال',\n",
       " 'ملاز',\n",
       " 'فی\\u200cالارض',\n",
       " 'لاله',\n",
       " 'لاتینو',\n",
       " 'گواتمالا',\n",
       " 'دلاویر',\n",
       " 'فیلادلفیا',\n",
       " 'اصلاحیه',\n",
       " 'متقابلا',\n",
       " 'ونزوئلا',\n",
       " 'لاب',\n",
       " 'دالاس',\n",
       " 'کلان\\u200cشهر',\n",
       " 'لانگ',\n",
       " 'آتلانتیک',\n",
       " 'لادردیل',\n",
       " 'آتلانتا',\n",
       " 'کلانشهر',\n",
       " 'بلاعوض',\n",
       " 'اقلا',\n",
       " 'فولاد',\n",
       " 'پالا',\n",
       " 'لا',\n",
       " 'بنگلاد',\n",
       " 'خلاق',\n",
       " 'گولا',\n",
       " 'کلان',\n",
       " 'بالاتر',\n",
       " 'لاابالی\\u200cگر',\n",
       " 'الافکار',\n",
       " 'اطلال',\n",
       " 'نیوکلاسیک',\n",
       " 'غلامحسن',\n",
       " 'جلال\\u200cالدین',\n",
       " 'بلاغ',\n",
       " 'غلامحسین',\n",
       " 'مثلااستخو',\n",
       " 'گلاب',\n",
       " 'سه\\u200cلایه\\u200cازجنس',\n",
       " 'غیراخلاق',\n",
       " 'اخلاق',\n",
       " 'تالار',\n",
       " 'اولاند',\n",
       " 'لاسکو',\n",
       " 'نیکولا',\n",
       " 'پرلاشز',\n",
       " 'پالاس',\n",
       " 'لاویل',\n",
       " 'فنلاند',\n",
       " 'ژولا',\n",
       " 'لاتویا',\n",
       " 'بلاروس',\n",
       " 'سالانه\\u200cاس',\n",
       " 'اصلاحات',\n",
       " 'صلاح',\n",
       " 'عادلانه',\n",
       " 'ناعادلانه',\n",
       " 'کلا',\n",
       " 'موزیلا',\n",
       " 'خلاء',\n",
       " 'پلانک',\n",
       " 'آلاینده',\n",
       " 'طلا',\n",
       " 'کلاه',\n",
       " 'علائ',\n",
       " 'عیلام',\n",
       " 'ایلامی',\n",
       " 'کلاه\\u200cتیزخود',\n",
       " 'ملازم',\n",
       " 'لاجورد',\n",
       " 'والا',\n",
       " 'املا',\n",
       " 'آنلاین',\n",
       " 'لاژورد',\n",
       " 'دولاب',\n",
       " 'روزآنلاین',\n",
       " 'اسلامیکا',\n",
       " 'نیلاندر',\n",
       " 'همشهری\\u200cآنلاین',\n",
       " 'کارلا',\n",
       " 'سلا',\n",
       " 'غلا',\n",
       " 'بلال',\n",
       " 'ربیع\\u200cالاول',\n",
       " 'خلافت',\n",
       " 'عاملان',\n",
       " 'جمادی\\u200cالاول',\n",
       " 'نهج\\u200cالبلاغه',\n",
       " 'الامک',\n",
       " 'مشکلات',\n",
       " 'پلاگین',\n",
       " 'غلاف',\n",
       " 'بالاس',\n",
       " 'قلاع',\n",
       " 'کهنسالان',\n",
       " 'سیلاخور',\n",
       " 'دالان',\n",
       " 'قزل\\u200cآلا',\n",
       " 'خاتم\\u200cالانبیا',\n",
       " 'نیکلا',\n",
       " 'بالاب',\n",
       " 'پالاینده',\n",
       " 'پالایه',\n",
       " 'طلال',\n",
       " 'الاقص',\n",
       " 'سینالا',\n",
       " 'اصطلاحا',\n",
       " 'چهارلا',\n",
       " 'سه\\u200cلا',\n",
       " 'گلابی\\u200cشکل',\n",
       " 'پلاستیک',\n",
       " 'لایه',\n",
       " 'لابمل',\n",
       " 'لاکرن',\n",
       " 'سکولاریس',\n",
       " 'موتورولا',\n",
       " 'محصولات',\n",
       " 'کلاهک',\n",
       " 'حملات',\n",
       " 'مسئولانه',\n",
       " 'لاتینی\\u200cساز',\n",
       " 'استیلا',\n",
       " 'لاییک',\n",
       " 'مالاکاس',\n",
       " 'لابئ',\n",
       " 'تاؤلان',\n",
       " 'باهیابلانکا',\n",
       " 'پلاتا',\n",
       " 'لاپلاتا',\n",
       " 'آدلاید',\n",
       " 'براتیسلاوا',\n",
       " 'ماچالا',\n",
       " 'لاشنجه',\n",
       " 'لایپزیگ',\n",
       " 'آنگولا',\n",
       " 'کامپالا',\n",
       " 'لاس',\n",
       " 'اکلاهماسیت',\n",
       " 'گلاسگو',\n",
       " 'لاپاز',\n",
       " 'لاهور',\n",
       " 'پوکاپالا',\n",
       " 'کالائو',\n",
       " 'دارالسلا',\n",
       " 'لانژو',\n",
       " 'ولادی\\u200cوستوک',\n",
       " 'سریلانکا',\n",
       " 'کالاما',\n",
       " 'سلاله',\n",
       " 'بارانکیلا',\n",
       " 'سانتاکلارا',\n",
       " 'دلاس',\n",
       " 'آخالکالاک',\n",
       " 'تلاو',\n",
       " 'مالابو',\n",
       " 'لائوس',\n",
       " 'مالاو',\n",
       " 'کوالالامپور',\n",
       " 'کازابلانکا',\n",
       " 'اولانباتار',\n",
       " 'گوادالاخارا',\n",
       " 'لاگوس',\n",
       " 'پورت\\u200cویلا',\n",
       " 'لاهه',\n",
       " 'سولا',\n",
       " 'علائم',\n",
       " 'اختلالات',\n",
       " 'الاءعلاق\\u200cالنفیسة',\n",
       " 'افلاک',\n",
       " 'لان',\n",
       " 'لانتانید',\n",
       " 'علاقه\\u200cدار',\n",
       " 'اتولا',\n",
       " 'ملاهاد',\n",
       " 'خانم\\u200cبالا',\n",
       " 'طلاق',\n",
       " 'بلا',\n",
       " 'لیلا',\n",
       " 'غلامحسین\\u200cمیرزا',\n",
       " 'جلال\\u200cالممالک',\n",
       " 'تحصیلات',\n",
       " 'لاغراندا',\n",
       " 'لابه\\u200cلا',\n",
       " 'گیلان',\n",
       " 'کلاغ',\n",
       " 'مولاناس',\n",
       " 'میرعلا',\n",
       " 'کلام',\n",
       " 'الا',\n",
       " 'اولا',\n",
       " 'استعلام',\n",
       " 'ملاحسین',\n",
       " 'جواهرالاسرار',\n",
       " 'زواهرالانوار',\n",
       " 'فاتح\\u200cالاب',\n",
       " 'علا',\n",
       " 'به\\u200cحلال',\n",
       " 'سپه\\u200cسالار',\n",
       " 'لاهج',\n",
       " 'لاین',\n",
       " 'جولانگاه',\n",
       " 'کلاسیس',\n",
       " 'کلاسیسیس',\n",
       " 'بالائ',\n",
       " 'هیمالایا',\n",
       " 'ماکس\\u200cپلانک',\n",
       " 'داگلاس',\n",
       " 'متلاش',\n",
       " 'الارب',\n",
       " 'کلارک',\n",
       " 'نولا',\n",
       " 'کلادون',\n",
       " 'کولادون',\n",
       " 'پلاستک',\n",
       " 'نورلامپ',\n",
       " 'موادلاز',\n",
       " 'لایه\\u200cنشان',\n",
       " 'لاشار',\n",
       " 'الاول',\n",
       " 'فلامینگو',\n",
       " 'وکلا',\n",
       " 'علاق',\n",
       " 'الإسلا',\n",
       " 'آلمانی\\u200cالاصل',\n",
       " 'ملاءعا',\n",
       " 'بالارتبه',\n",
       " 'شلاق',\n",
       " 'لاور',\n",
       " 'ملایر',\n",
       " 'ملامحمد',\n",
       " 'لائودیسه',\n",
       " 'لااودیسه',\n",
       " 'فتوکلاژ',\n",
       " 'لاهیجان',\n",
       " 'خلایق',\n",
       " 'الاشرف',\n",
       " 'کربلا',\n",
       " 'معلا',\n",
       " 'الاقال',\n",
       " 'الاسفزار',\n",
       " 'دارالخلافه',\n",
       " 'گیلاس',\n",
       " 'اعتلاء',\n",
       " 'اعتلا',\n",
       " 'سیلاب',\n",
       " 'بالاسر',\n",
       " 'پالاندوز',\n",
       " 'ییلاق',\n",
       " 'دارالولایه',\n",
       " 'جوادالائمه',\n",
       " 'خاتم\\u200cالانبیاء',\n",
       " 'اجلاس',\n",
       " 'ثامن\\u200cالائمه',\n",
       " 'کلانتر',\n",
       " 'کمپوستلا',\n",
       " 'غلام\\u200cحسین',\n",
       " 'باب\\u200cالابواب',\n",
       " 'سولاک',\n",
       " 'خبرآنلاین',\n",
       " 'سفلا',\n",
       " 'کشورعملا',\n",
       " 'پلانکتون',\n",
       " 'لاکادیو',\n",
       " 'لاکشادویپ',\n",
       " 'میلا',\n",
       " 'سلاجقه',\n",
       " 'ملازگرد',\n",
       " 'جلال\\u200cالدوله',\n",
       " 'الابنیه',\n",
       " 'الادویه',\n",
       " 'قشلاق',\n",
       " 'بلافصل',\n",
       " 'اسلام\\u200cآباد',\n",
       " 'غلامشاه',\n",
       " 'شلا',\n",
       " 'گلابتون\\u200cدوز',\n",
       " 'هلاج',\n",
       " 'حلاج',\n",
       " 'ملاکاوو',\n",
       " 'کلاقاه',\n",
       " 'کلاو',\n",
       " 'پلان',\n",
       " 'امجدالاشراف',\n",
       " 'شیخ\\u200cالاسلا',\n",
       " 'ملالطف',\n",
       " 'دارالاحس',\n",
       " 'دارالا',\n",
       " 'قدیم\\u200cالایا',\n",
       " 'دارالایاله',\n",
       " 'محلات',\n",
       " 'صدرالاشراف',\n",
       " 'الاعتقاد',\n",
       " 'الاسلام',\n",
       " 'فلاسفه',\n",
       " 'الاه',\n",
       " 'عقلان',\n",
       " 'خلاقانه',\n",
       " 'سؤالات',\n",
       " 'جملات',\n",
       " 'بلایا',\n",
       " 'جلا',\n",
       " 'ولانس',\n",
       " 'جلالوند',\n",
       " 'کلاوه',\n",
       " 'بیلا',\n",
       " 'بلایای',\n",
       " 'سری\\u200cلانکا',\n",
       " 'لانکا',\n",
       " 'پالار',\n",
       " 'عیلا',\n",
       " 'هیولا',\n",
       " 'اصولا',\n",
       " 'فولادزره',\n",
       " 'بوسلامه',\n",
       " 'امیدسالار',\n",
       " 'الإسلامیة',\n",
       " 'ملاقات',\n",
       " 'آق\\u200cقلا',\n",
       " 'آلا',\n",
       " 'فلاورج',\n",
       " 'ثلاث',\n",
       " 'اسلام\\u200cشهر',\n",
       " 'اسلامشهر',\n",
       " 'اسلامیه',\n",
       " 'لامرد',\n",
       " 'اصلاندوز',\n",
       " 'امیرکلا',\n",
       " 'لارس',\n",
       " 'بالاده',\n",
       " 'چالانچول',\n",
       " 'رستمکلا',\n",
       " 'سرخنکلاته',\n",
       " 'علامرود',\n",
       " 'فولادشهر',\n",
       " 'کلاته',\n",
       " 'کلاچا',\n",
       " 'کلارآباد',\n",
       " 'کلارد',\n",
       " 'کلاله',\n",
       " 'کیاکلا',\n",
       " 'گیلانغرب',\n",
       " 'لار',\n",
       " 'لالجین',\n",
       " 'لال',\n",
       " 'لاهرود',\n",
       " 'مرزیکلا',\n",
       " 'معلم\\u200cکلایه',\n",
       " 'ملاثان',\n",
       " 'ملارد',\n",
       " 'میلاجرد',\n",
       " 'جلایر',\n",
       " 'بلاذر',\n",
       " 'فاضلاب',\n",
       " 'حلال',\n",
       " 'الامر',\n",
       " 'ملامراد',\n",
       " 'لایحضره',\n",
       " 'الانوارالنعمانیه',\n",
       " 'دررالاشعار',\n",
       " 'مصلا',\n",
       " 'کلاهخود',\n",
       " 'الاصل',\n",
       " 'دلاویل',\n",
       " 'فیلارمونیک',\n",
       " 'ساپودیلا',\n",
       " 'اسطرلاب',\n",
       " 'زیلان',\n",
       " 'لاخ',\n",
       " 'دراکولا',\n",
       " 'کوپولا',\n",
       " 'لایکن',\n",
       " 'بالابردن',\n",
       " 'نیلا',\n",
       " 'بالابلند',\n",
       " 'بلندبالا',\n",
       " 'پائولا',\n",
       " 'وبلاگ',\n",
       " 'کتابلاگ',\n",
       " 'ملام',\n",
       " 'متلاط',\n",
       " 'آلاشکر',\n",
       " 'املائ',\n",
       " 'لازار',\n",
       " 'کالال',\n",
       " 'کالاآل',\n",
       " 'کالالیت',\n",
       " 'توپیلاک',\n",
       " 'کلال',\n",
       " 'پردلانه',\n",
       " 'اساس\\u200cالاقتباس',\n",
       " 'کلاما',\n",
       " 'اسلامی\\u200cس',\n",
       " 'هلاکو',\n",
       " 'اعلاء',\n",
       " 'الاعراق',\n",
       " 'هلاکوخ',\n",
       " 'البلاغه',\n",
       " 'البلاغة',\n",
       " 'الالقاب',\n",
       " 'رسالات',\n",
       " 'الاقتباس',\n",
       " 'ذیلا',\n",
       " 'الاختیار',\n",
       " ';کلا',\n",
       " 'تجریک\\u200cالکلا',\n",
       " 'بروکلاین',\n",
       " 'بولاک',\n",
       " 'کلایو',\n",
       " 'اصلان',\n",
       " 'آن\\u200cلا',\n",
       " 'ستلا',\n",
       " 'ماتلاک',\n",
       " 'پورتلاندیا',\n",
       " 'پدرسالار',\n",
       " 'بی\\u200cاطلاع',\n",
       " 'لاک\\u200cپ',\n",
       " 'کارتالا',\n",
       " 'الببلاو',\n",
       " 'الازهر',\n",
       " 'اسلامی\\u200cساز',\n",
       " 'الاحمر',\n",
       " 'مردسالار',\n",
       " 'یونانی\\u200cالاصل',\n",
       " 'ارلاخ',\n",
       " 'ازمیلاد',\n",
       " 'کلاست',\n",
       " 'علاءالدین',\n",
       " 'علا\\u200cالدین',\n",
       " 'علاالدین',\n",
       " 'لاریج',\n",
       " 'دیولافوا',\n",
       " 'مداخلات',\n",
       " 'گیلانی',\n",
       " 'محتملا',\n",
       " 'لاره',\n",
       " 'لاریجان',\n",
       " 'غلامعل',\n",
       " 'داعی\\u200cالاسلا',\n",
       " 'غلام',\n",
       " 'الاما',\n",
       " 'فخرالاسلا',\n",
       " 'علاو',\n",
       " 'چلاب',\n",
       " 'الاصغر',\n",
       " 'دلارستاق',\n",
       " 'ملاط',\n",
       " 'اضلاع',\n",
       " 'فیروزکلا',\n",
       " 'علاءالدوله',\n",
       " 'تلار',\n",
       " 'هندوکلا',\n",
       " 'قاضی\\u200cکلا',\n",
       " 'چلاو',\n",
       " 'آلاچیق',\n",
       " 'آلامل',\n",
       " 'پلاژ',\n",
       " 'لاک',\n",
       " 'تعطیلات',\n",
       " 'افلاطون',\n",
       " 'پلاتون',\n",
       " 'افلاطون\\u200cشناس',\n",
       " 'گلاوکن',\n",
       " 'افلاطون\\u200cگرا',\n",
       " 'کولاب',\n",
       " 'شلایشر',\n",
       " 'کلارا',\n",
       " 'پاولا',\n",
       " 'ایالات',\n",
       " 'منفعلانه',\n",
       " 'علایق',\n",
       " 'قلاده',\n",
       " 'بلاند',\n",
       " 'آنگلا',\n",
       " 'لابراتوار',\n",
       " 'بلاواتسک',\n",
       " 'ضداطلاع',\n",
       " 'فالانژیس',\n",
       " 'مسجدالاقص',\n",
       " 'لاشه',\n",
       " 'لازمه',\n",
       " 'مشرق\\u200cالاذکار',\n",
       " 'استقلال\\u200cطلبانه',\n",
       " 'گه\\u200cلاویژ',\n",
       " 'گه\\u200cلارێز',\n",
       " 'گه\\u200cلاوێژ',\n",
       " 'لاویژ',\n",
       " 'لائیک',\n",
       " 'کلاسوس',\n",
       " 'اسلامبول',\n",
       " 'سکولاریز',\n",
       " 'مردمسالار',\n",
       " 'آلانیا',\n",
       " 'گالاتاسرا',\n",
       " 'مالاتیا',\n",
       " 'موغلا',\n",
       " 'انگولا',\n",
       " 'گوآتمالا',\n",
       " 'فلاند',\n",
       " 'توکلائو',\n",
       " 'ملانز',\n",
       " 'پالائو',\n",
       " 'پولاد',\n",
       " 'هاسپولادل',\n",
       " 'باجلان',\n",
       " 'مستقلا',\n",
       " 'خلاصه\\u200cشده',\n",
       " 'لازلو',\n",
       " 'لاپتون',\n",
       " 'خلاقه',\n",
       " 'بلامنازع',\n",
       " 'علامت\\u200cگذاری',\n",
       " 'املاک',\n",
       " 'علامت',\n",
       " 'طلاساز',\n",
       " 'الازیغ',\n",
       " 'لاشکار',\n",
       " 'لانه',\n",
       " 'آزاداسلام',\n",
       " 'سلامت',\n",
       " 'ازاسلا',\n",
       " 'جلال\\u200cآباد',\n",
       " 'لاله\\u200cزار',\n",
       " 'مولا',\n",
       " 'شهلا',\n",
       " 'متوسلان',\n",
       " 'کاپولا',\n",
       " 'قیزلاردفه',\n",
       " 'غلات',\n",
       " 'آلاداغ',\n",
       " 'فالاریس',\n",
       " 'قلا',\n",
       " 'بولارسنگ',\n",
       " 'کلاودیوس',\n",
       " 'کوجولا',\n",
       " 'مایملاک',\n",
       " 'کالاله',\n",
       " 'بلاوقفه',\n",
       " 'ویلا',\n",
       " 'بالاحصار',\n",
       " 'عبدالسلا',\n",
       " 'دهلا',\n",
       " 'لابه',\n",
       " 'حلاو',\n",
       " 'مقالات',\n",
       " 'لارستان',\n",
       " 'خلا',\n",
       " 'الانبار',\n",
       " 'حقلانیه',\n",
       " 'کربلاء',\n",
       " 'الاخیضر',\n",
       " 'الاقیصر',\n",
       " 'الامیر',\n",
       " 'وادی\\u200cالسلا',\n",
       " 'بنه\\u200cسلاوه',\n",
       " 'شقلاوه',\n",
       " 'باتلاق',\n",
       " 'بلاروسی',\n",
       " 'لاکهار',\n",
       " 'اسلاوی\\u200cزب',\n",
       " 'یاروسلاو',\n",
       " 'یاروسلاول',\n",
       " 'لاستیک',\n",
       " 'ابتلا',\n",
       " 'لایلا',\n",
       " 'والاگرا',\n",
       " 'اختلاف\\u200cنظر',\n",
       " 'بلام',\n",
       " 'غیرانقلاب',\n",
       " 'ختلان',\n",
       " 'ختلانى',\n",
       " 'فعالان',\n",
       " 'زلال',\n",
       " 'قوبلا',\n",
       " 'الاغ',\n",
       " 'مارگولاشویل',\n",
       " 'زاکاتالا',\n",
       " 'تحت\\u200cالارض',\n",
       " 'لوگلا',\n",
       " 'آلاس',\n",
       " 'لاینفک',\n",
       " 'آلازان',\n",
       " 'فعالانه',\n",
       " 'لاریس',\n",
       " 'جلالکوټ',\n",
       " 'پولاک',\n",
       " 'پلارو',\n",
       " 'لاج',\n",
       " 'الاهه',\n",
       " 'کلام\\u200cشناس',\n",
       " 'علامه\\u200cزاده',\n",
       " 'والاتبار',\n",
       " 'سالاریه',\n",
       " 'کولا',\n",
       " 'طلاو',\n",
       " 'ملامجدالدین',\n",
       " 'لادبن',\n",
       " 'همولایت',\n",
       " 'لاک\\u200cتراش',\n",
       " 'کیله\\u200cلاک',\n",
       " 'لالا',\n",
       " 'مبتلاس',\n",
       " 'ملاعبد',\n",
       " 'میکولا',\n",
       " 'اختلاس',\n",
       " 'بالاک\\u200cحصار',\n",
       " 'پولاتل',\n",
       " 'یایلادره',\n",
       " 'اخلاط',\n",
       " 'آق\\u200cلاسون',\n",
       " 'آلتین\\u200cیایلا',\n",
       " 'آتکاراجالار',\n",
       " 'فالابلا',\n",
       " 'ملانوکورتین',\n",
       " 'لامسه',\n",
       " 'لاغر',\n",
       " 'عضلان',\n",
       " 'کلایدزدیل',\n",
       " 'ویولا',\n",
       " 'میرجلال',\n",
       " 'افلان',\n",
       " 'اضمحلال',\n",
       " 'آلاجه\\u200cکایه',\n",
       " 'کوونجیلار',\n",
       " 'باش\\u200cیایلا',\n",
       " 'سولاک\\u200cیور',\n",
       " 'دالا',\n",
       " 'میلاس',\n",
       " 'بولانیک',\n",
       " 'مجدالادباء',\n",
       " 'کلاسه',\n",
       " 'ملاذوالفقار',\n",
       " 'تولا',\n",
       " 'تالاب',\n",
       " 'شایسته\\u200cسالار',\n",
       " 'خویشاوندسالار',\n",
       " 'خلاقیت',\n",
       " 'برملا',\n",
       " 'الام',\n",
       " 'قلاب',\n",
       " 'اطلاعیه',\n",
       " 'جلاله',\n",
       " 'نیکولاس',\n",
       " 'طلائ',\n",
       " 'شیلات',\n",
       " 'فلاح',\n",
       " 'آیدینلار',\n",
       " 'آکینجیلار',\n",
       " 'شارقشلا',\n",
       " 'داغلار',\n",
       " 'یایلاجیک',\n",
       " 'آسیمیلاسیون',\n",
       " 'ریونکلا',\n",
       " 'اسکولاستیک',\n",
       " 'پرکلاغ',\n",
       " 'تریلان',\n",
       " 'اسلاگهورن',\n",
       " 'لائ',\n",
       " 'قافلان\\u200cکوه',\n",
       " 'لابا',\n",
       " 'طلایی\\u200cرنگ',\n",
       " 'نبوپولاسار',\n",
       " 'بالاپوش',\n",
       " 'عیلامی',\n",
       " 'آلدلاین',\n",
       " 'بالارفتن',\n",
       " 'لامپ',\n",
       " 'ملاابراه',\n",
       " 'ملازاهد',\n",
       " 'ناوقلا',\n",
       " 'مه\\u200cلاکه\\u200cر',\n",
       " 'ملک\\u200cالکلا',\n",
       " 'مولان\\u200cآباد',\n",
       " 'اردلانیه',\n",
       " 'مولاناآباد',\n",
       " 'استالاکت',\n",
       " 'استالاگم',\n",
       " 'قالاس',\n",
       " 'ملاداغ',\n",
       " 'بالاجوجه',\n",
       " 'السلام',\n",
       " 'پیرقشلاق',\n",
       " 'قزلار',\n",
       " 'گلابر',\n",
       " 'لاشه\\u200cاس',\n",
       " 'بولاماج',\n",
       " 'کهلا',\n",
       " 'درتلا',\n",
       " 'بلاتریکس',\n",
       " 'اسلاگهورون',\n",
       " 'ایرانی\\u200cالاصل',\n",
       " 'بلااستفاده',\n",
       " 'لاکهید',\n",
       " 'علاقمند',\n",
       " 'عقلا',\n",
       " 'علی\\u200cالاصول',\n",
       " 'کلایپدا',\n",
       " 'یوگسلاو',\n",
       " 'ویاچسلاو',\n",
       " 'پولادین',\n",
       " 'یوگوسلاو',\n",
       " 'همکلاسی',\n",
       " 'ویستولا',\n",
       " 'بلاشرط',\n",
       " 'لانگریچ',\n",
       " 'تلاند',\n",
       " 'ملاحظ',\n",
       " 'راینلاند',\n",
       " 'فولادساز',\n",
       " 'کولاک',\n",
       " 'دلانو',\n",
       " 'لاو',\n",
       " 'فریدلاندر',\n",
       " 'خردسالان',\n",
       " 'استلا',\n",
       " 'پلاکارد',\n",
       " 'لاگارد',\n",
       " 'مارین\\u200cپلاتز',\n",
       " 'اخلاقی',\n",
       " 'خلافکار',\n",
       " 'تحولات',\n",
       " 'بالادس',\n",
       " 'لائورل',\n",
       " 'ماژلان',\n",
       " 'کلاؤس',\n",
       " 'اخلال',\n",
       " 'میرلاشار',\n",
       " 'برکلاه',\n",
       " 'آلان',\n",
       " 'زولاند',\n",
       " 'گلاسل',\n",
       " 'الاحمد',\n",
       " 'الامه',\n",
       " 'الانباء',\n",
       " 'علامت\\u200cگذار',\n",
       " 'تسهیلات',\n",
       " 'المطلاع',\n",
       " 'اشکالات',\n",
       " 'الاعظ',\n",
       " 'الاربعون',\n",
       " 'الاعتقادیه',\n",
       " 'الالفیة',\n",
       " 'الاخوین',\n",
       " 'الارشاد',\n",
       " 'خلاصة',\n",
       " 'الاعتبار',\n",
       " 'الاعتمار',\n",
       " 'الامامیه',\n",
       " 'الاصول',\n",
       " 'لازمس',\n",
       " 'بالاسان',\n",
       " 'آسلاماز',\n",
       " 'جولفالاک',\n",
       " 'دوسالانه',\n",
       " 'معاملات',\n",
       " 'سوتلانا',\n",
       " 'لاوال',\n",
       " 'الاب',\n",
       " 'طلایه',\n",
       " 'تذکرةالاولیاء',\n",
       " 'لب\\u200cالالباب',\n",
       " 'الافاضل',\n",
       " 'الاصفیا',\n",
       " 'نفحات\\u200cالانس',\n",
       " 'الانس',\n",
       " 'دلا',\n",
       " 'صلا',\n",
       " 'تذکرةالاولیا',\n",
       " 'هیلاج\\u200cنامه',\n",
       " 'پلاسما',\n",
       " 'دویچلاند',\n",
       " 'پلاتسک',\n",
       " 'رولاند',\n",
       " 'کلاوستال',\n",
       " 'زاورلاند',\n",
       " 'لاند',\n",
       " 'زیگرلاند',\n",
       " 'گلادباخ',\n",
       " 'مونشن\\u200cگلادباخ',\n",
       " 'راین\\u200cلاند',\n",
       " 'لانگن\\u200cبرگ',\n",
       " 'مونسترلاند',\n",
       " 'لامذهب',\n",
       " 'علاوه\\u200cبر',\n",
       " 'کایزرسلاوترن',\n",
       " 'زارلاند',\n",
       " 'اسلامگرائ',\n",
       " 'لایپنیتز',\n",
       " 'استانیسلاو',\n",
       " 'پلاون',\n",
       " 'اعتلاف',\n",
       " 'اورلا',\n",
       " 'سرکلایس',\n",
       " 'لانگوبارد',\n",
       " 'تسلا',\n",
       " 'بلاصاحب',\n",
       " 'دوحهٔ\\u200cالازهار',\n",
       " 'جنهٔ\\u200cالاثمار',\n",
       " 'زینهٔ\\u200cالاوراق',\n",
       " 'صحیفهٔ\\u200cالاخلاص',\n",
       " 'لاریب',\n",
       " 'مروج\\u200cالاسواق',\n",
       " 'مهیج\\u200cالاشواق',\n",
       " 'نهایهٔ\\u200cالاعجاز',\n",
       " 'مظهرالاسرار',\n",
       " 'الازهار',\n",
       " 'الاسرار',\n",
       " 'الاثمار',\n",
       " 'الاوراق',\n",
       " 'الاخلاص',\n",
       " 'بلاروسکایا',\n",
       " 'مالا',\n",
       " 'ولاپوک',\n",
       " 'ولاپوکvolapuk',\n",
       " 'انگلولاتین',\n",
       " 'زیرلایه',\n",
       " 'موصولانلو',\n",
       " 'آلاگوز',\n",
       " 'آقبلاغ',\n",
       " 'کالار',\n",
       " 'جلاد',\n",
       " 'کلار',\n",
       " 'کثیرالانتشار',\n",
       " 'کلاسیک\\u200cسرا',\n",
       " 'علامات',\n",
       " 'صلاح\\u200cالدین',\n",
       " 'انصارالاسلا',\n",
       " 'شیخلا',\n",
       " 'لامبدا',\n",
       " 'لاورنس',\n",
       " 'جولاندز',\n",
       " 'فلاستمد',\n",
       " 'لاپلاس',\n",
       " 'اوبلاکر',\n",
       " 'پلارد',\n",
       " 'اسپالاسیون',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    # WARNING: not based on slides     \n",
    "    words = []\n",
    "    for word in positional_index:\n",
    "        if bigram in word:\n",
    "            words.append(word)\n",
    "    return words\n",
    "get_words_with_bigram('لا')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    # WARNING: based on slides    \n",
    "    inverted_index = {}\n",
    "    for word in positional_index:\n",
    "        word_length = len(word)\n",
    "        if word_length >= 2:\n",
    "            for i in range(len(word) - 2):\n",
    "                bi = word[i: i + 2]\n",
    "                if bi not in inverted_index:\n",
    "                    inverted_index[bi] = []\n",
    "                if word not in inverted_index[bi]:\n",
    "                    inverted_index[bi].append(word)\n",
    "    return inverted_index[bigram]\n",
    "                    \n",
    "get_words_with_bigram('لا')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    address = docs_path + '/' + doc_num + '.xml'\n",
    "    construct_positional_indexes(address)\n",
    "\n",
    "add_document_to_indexes('data/wiki', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    address = docs_path + '/' + doc_num + '.xml'\n",
    "    dom = parse(address)\n",
    "    pages = dom.getElementsByTagName('page')    \n",
    "    for page in pages:\n",
    "        title = get_title(page.getElementsByTagName('title'))\n",
    "        text = get_text(page.getElementsByTagName('text'))\n",
    "        doc_id = get_id(page.getElementsByTagName('id'))\n",
    "        \n",
    "        text_words_list = prepare_text(text)\n",
    "        title_words_list = prepare_text(title)\n",
    "        \n",
    "        for index, text_word in enumerate(text_words_list):\n",
    "            if text_word not in positional_index:\n",
    "                print('کلمه پیدا نشد')\n",
    "            elif doc_id not in positional_index[text_word]:\n",
    "                print('چنین کلمه‌ای با این شماره سند موجود نیست')\n",
    "            else:\n",
    "                positional_index[text_word].remove(doc_id)\n",
    "            if len(positional_index[text_word]) == 0:\n",
    "                del positional_index[text_word]\n",
    "                \n",
    "        for index, title_word in enumerate(title_words_list):\n",
    "            if title_word not in positional_index:\n",
    "                print('کلمه پیدا نشد')\n",
    "            elif doc_id not in positional_index[title_word]:\n",
    "                print('چنین کلمه‌ای با این شماره سند موجود نیست')\n",
    "            else:\n",
    "                positional_index[title_word].remove(doc_id)\n",
    "            if len(positional_index[title_word]) == 0:\n",
    "                del positional_index[title_word]\n",
    "\n",
    "delete_document_from_indexes('data/wiki', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akbar = {'kalame': {}, 'nokaram': {}}\n",
    "if len(akbar['kalame']) == 0:\n",
    "    del akbar['kalame']\n",
    "akbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    full_destination = destination + '.json'\n",
    "    with open(full_destination, 'w') as f:\n",
    "        json.dump(positional_index, f, ensure_ascii=False)\n",
    "\n",
    "save_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['salam ', 'chetori haji', ' khoobi?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'salam \"chetori haji\" khoobi?'\n",
    "b = a.split(\"\\\"\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_queries(directory):\n",
    "    queries = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open (os.path.join(directory, filename), 'r') as f:\n",
    "            query = f.readline()\n",
    "            queries.append(query)\n",
    "    return queries\n",
    "queries = get_all_queries('project1_data/data/queries/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['طبیعت', 'دامنه', 'کوه', 'ایرانی'])\n",
      "(['علوم اجتماعی'], ['مطالعه', 'در', 'دانشگاه'])\n",
      "([], ['هیتلر', 'در', 'جنگ', 'جهانی', 'اول'])\n",
      "(['منظومه شمسی'], ['سیاره', 'های', 'بزرگ', '\\n'])\n",
      "([], ['جنگل', 'های', 'بلوط', 'ایران'])\n",
      "([], ['زندگی', 'حیوانات', 'وحشی'])\n",
      "([], ['مسابقات', 'فوتبال', 'المپیک'])\n",
      "([], ['کشورهای', 'عضو', 'اتحادیه', 'آفریقا'])\n",
      "([], ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجوان'])\n",
      "([], ['برنده', 'جایزه', 'بهترین', 'فیلم', 'در', 'جشنواره'])\n",
      "([], ['ابزار', 'های', 'فضایی', 'و', 'پیشرفته', 'ناسا'])\n",
      "([], ['سواحل', 'دریای', 'سرخ'])\n",
      "(['خلیج فارس'], ['پایتخت', 'کشورهای', 'حوزه', ''])\n",
      "([], ['کشورهای', 'دارای', 'نفت', 'در', 'خاورمینا'])\n",
      "([], ['انتخابات', 'نمایندگان', 'مجلس', 'ایالتی', 'در', 'آمریکا'])\n",
      "([], ['تاریخچه', 'هنر', 'نمایشی', 'در', 'ایران'])\n",
      "(['باشگاه فوتبال'], ['', 'اروپایی'])\n",
      "([], ['تاریخ', 'علوم', 'اجتماعی', 'در', 'اروپا'])\n",
      "([], ['جاذبه', 'گردشگری', 'در', 'استان', 'کردستان'])\n",
      "([], ['درمان', 'بیماری', 'افسردگی'])\n"
     ]
    }
   ],
   "source": [
    "def parsing_query(query):\n",
    "    double_quoted = re.findall('\"([^\"]*)\"', query)\n",
    "    query = query.replace('\"', \"\")\n",
    "    for quoted in double_quoted:\n",
    "        query = query.replace(quoted, \"\")\n",
    "        query = query.replace('  ', \" \")\n",
    "    not_sequential = query.split(\" \")\n",
    "    return double_quoted, not_sequential\n",
    "\n",
    "# double_quoted, not_sequential = parsing_query('سلام مردم \"عزیز ایران\" به برنامه \"خندوانه هندوانه\" خوش آمدید')\n",
    "for query in queries:\n",
    "    print(parsing_query(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.284132903648771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'7145': 0.29194975058406386, '7156': 0.95643365851161}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(doc_length)\n",
    "\n",
    "def get_dictionary(query_list):\n",
    "    dic = {}\n",
    "    for term in query_list:\n",
    "        if term not in dic:\n",
    "            dic[term] = 1\n",
    "        else:\n",
    "            dic[term] += 1\n",
    "    return dic\n",
    "\n",
    "\n",
    "def consine_score(query_list, method, weight):\n",
    "    scores = {} #score of documents\n",
    "    terms_occurence = get_dictionary(query_list) #terms occurence in query\n",
    "    terms = list(terms_occurence.keys()) #all distinct terms in query\n",
    "        \n",
    "    for term in terms_occurence:\n",
    "        w_tq = 1 + math.log10(terms_occurence[term])\n",
    "        \n",
    "        if term in positional_index:\n",
    "            docs_list = positional_index[term]\n",
    "            for doc in docs_list:\n",
    "                document_occurence = len(docs_list[doc])\n",
    "                idf = math.log(N / len(positional_index[term]))\n",
    "                tf = 1 + math.log10(document_occurence)\n",
    "                w_td = tf * idf\n",
    "        else:\n",
    "            w_td = 0\n",
    "            \n",
    "        if doc in scores:\n",
    "            scores[doc] += w_tq * w_td\n",
    "        else:\n",
    "            scores[doc] = w_tq * w_td\n",
    "    \n",
    "    if method == 'ltc-lnc':\n",
    "        normalization = 0\n",
    "        for doc in scores:\n",
    "            score = scores[doc]\n",
    "            normalization += (score * score)\n",
    "        normalization = math.sqrt(normalization)\n",
    "        print(normalization)\n",
    "        for doc in scores:\n",
    "            scores[doc] /= normalization\n",
    "    return scores\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    double_quoted, not_sequential = parsing_query(query)\n",
    "    if not double_quoted:\n",
    "        prepared_query = prepare_text(query)\n",
    "        return consine_score(prepared_query, method, weight)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "search(queries[0], 'ltc-lnc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': ['10'], 'text': []}\n"
     ]
    }
   ],
   "source": [
    "positional_index = {\n",
    "    'خلیج': {\n",
    "        '1': {\n",
    "            'title': [1, 4, 8],\n",
    "            'text': [2, 5, 9]\n",
    "        },\n",
    "        '10': {\n",
    "            'title': [3],\n",
    "            'text': [12]\n",
    "        }\n",
    "    },\n",
    "    'فارس': {\n",
    "        '10': {\n",
    "            'title': [4],\n",
    "            'text': [13]\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    'ایران': {\n",
    "        '10' {\n",
    "            'title': [5],\n",
    "            'text': [9]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def positional_intersect(phrase, weight):\n",
    "    phrases = prepare_text(phrase)\n",
    "    phrase_touples = []\n",
    "    \n",
    "    for i in range(len(phrases) - 1):\n",
    "        phrase_touples.append((phrases[i], phrases[i + 1]))\n",
    "    \n",
    "    answer = {}\n",
    "    for index, (phrase1, phrase2) in phrase_touples:\n",
    "        try:\n",
    "            doc_list1 = positional_index[phrase1]\n",
    "            doc_list2 = positional_index[phrase2]\n",
    "            doc_list_len1 = len(doc_list1)\n",
    "            doc_list_len2 = len(doc_list2)\n",
    "\n",
    "            i, j = 0, 0\n",
    "            while i < doc_list_len1 and j < doc_list_len2:\n",
    "                doc1_keys = list(doc_list1.keys())\n",
    "                doc2_keys = list(doc_list2.keys())\n",
    "\n",
    "                doc1_key = doc1_keys[i]\n",
    "                doc2_key = doc2_keys[j]\n",
    "\n",
    "                if doc1_key == doc2_key:\n",
    "                    flag = True\n",
    "                    doc_id = doc1_keys[i]\n",
    "                    m, n = 0, 0\n",
    "                    contexts = ['title', 'text']\n",
    "                    for context in contexts:\n",
    "                        try:\n",
    "                            while m < len(doc_list1[doc1_key][context]) and n < len(doc_list2[doc2_key][context]):\n",
    "                                first_phrase_index = doc_list1[doc1_key][context][m]\n",
    "                                second_phrase_index = doc_list2[doc2_key][context][n]\n",
    "                                if second_phrase_index - first_phrase_index == 1:\n",
    "                                    answer[context].append(doc_id)\n",
    "                                    m += 1\n",
    "                                    n += 1\n",
    "                                elif first_phrase_index < second_phrase_index:\n",
    "                                    m += 1\n",
    "                                else:\n",
    "                                    n += 1\n",
    "                        except:\n",
    "                            continue\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "\n",
    "\n",
    "                elif doc1_key < doc2_key:\n",
    "                    i += 1\n",
    "                else:\n",
    "                    j += 1\n",
    "\n",
    "            return answer\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "print(positional_intersect('خلیج', 'فارس', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('persian', 'gulf'), ('gulf', 'iran')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-29e1237f7aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mpositional_intersect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'persian gulf iran'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-161-29e1237f7aa1>\u001b[0m in \u001b[0;36mpositional_intersect\u001b[0;34m(phrase, weight)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase_touples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mphrase1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphrase_touples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "positional_index = {\n",
    "    'خلیج': {\n",
    "        '1': {\n",
    "            'title': [1, 4, 8],\n",
    "            'text': [2, 5, 9]\n",
    "        },\n",
    "        '10': {\n",
    "            'title': [3],\n",
    "            'text': [12]\n",
    "        }\n",
    "    },\n",
    "    'فارس': {\n",
    "        '10': {\n",
    "            'title': [4],\n",
    "            'text': [13]\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    'ایر': {\n",
    "        '10': {\n",
    "            'title': [5],\n",
    "            'text': [9]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def positional_intersect(phrase, weight):\n",
    "\n",
    "    phrases = prepare_text(phrase)\n",
    "    phrase_touples = []\n",
    "    \n",
    "    for i in range(len(phrases) - 1):\n",
    "        phrase_touples.append((phrases[i], phrases[i + 1]))\n",
    "    \n",
    "    print(phrase_touples)\n",
    "    answer = {}\n",
    "    for index, (phrase1, phrase2) in phrase_touples:\n",
    "        if index == 0:\n",
    "            try:\n",
    "                doc_list1 = positional_index[phrase1]\n",
    "                doc_list2 = positional_index[phrase2]\n",
    "                doc_list_len1 = len(doc_list1)\n",
    "                doc_list_len2 = len(doc_list2)\n",
    "                \n",
    "                i, j = 0, 0\n",
    "                while i < doc_list_len1 and j < doc_list_len2:\n",
    "                    doc1_keys = list(doc_list1.keys())\n",
    "                    doc2_keys = list(doc_list2.keys())\n",
    "                    \n",
    "                    doc1_key = doc1_keys[i]\n",
    "                    doc2_key = doc2_keys[j]\n",
    "                    \n",
    "                    if doc1_key == doc2_key:\n",
    "                        doc_id = doc1_key\n",
    "                        m, n = 0, 0\n",
    "                        contexts = ['title', 'text']\n",
    "                        for context in contexts:\n",
    "                            try:\n",
    "                                while m < len(doc_list1[doc1_key][context]) and n < len(doc_list2[doc2_key][context]):\n",
    "                                    first_phrase_index = doc_list1[doc1_key][context][m]\n",
    "                                    second_phrase_index = doc_list2[doc_key][context][n]\n",
    "                                    if second_phrase_index - first_phrase_index == 1:\n",
    "                                        if doc_id not in answer:\n",
    "                                            answer[doc_id] = {}\n",
    "\n",
    "                                        if context not in answer[doc_id]:\n",
    "                                            answer[doc_id][context] = []\n",
    "                                        \n",
    "                                        answer[doc_id][context].append(second_phrase_index)\n",
    "                                        m += 1\n",
    "                                        n += 1\n",
    "                                    elif first_phrase_index < second_phrase_index:\n",
    "                                        m += 1\n",
    "                                    else:\n",
    "                                        n += 1\n",
    "                                        \n",
    "                            except:\n",
    "                                continue\n",
    "                                            \n",
    "            except:\n",
    "                continue\n",
    "        else:\n",
    "            if answer != {}:\n",
    "                new_answer = {}\n",
    "                doc_list1 = answer\n",
    "                doc_list2 = positional_index[phrase2]\n",
    "                doc_list_len1 = len(answer)\n",
    "                doc_list_len2 = len(doc_list2)\n",
    "                i, j = 0, 0\n",
    "                while i < doc_list_len1 and j < doc_list_len2:\n",
    "                    doc1_keys = doc_list1.keys()\n",
    "                    doc2_keys = doc_list2.keys()\n",
    "                    \n",
    "                    doc1_key = doc1_keys[i]\n",
    "                    doc2_key = doc2_keys[j]\n",
    "                    \n",
    "                    if doc1_key == doc2_key:\n",
    "                        doc_id = doc1_key\n",
    "                        m, n = 0, 0\n",
    "                        contexts = ['title', 'text']\n",
    "                        for context in contexts:\n",
    "                            try:\n",
    "                                while m < len(doc_list1[doc1_key][context]) and n < len(doc_list2[doc2_key][context]):\n",
    "                                    first_phrase_index = doc_list1[doc1_key][context][m]\n",
    "                                    second_phrase_index = doc_list2[doc_key][context][n]\n",
    "                                    if second_phrase_index - first_phrase_index == 1:\n",
    "                                        if doc_id not in new_answer:\n",
    "                                            new_answer[doc_id] = {}\n",
    "                                        if context not in new_answer[doc_id]:\n",
    "                                            new_answer[doc_id][context] = []\n",
    "                                            \n",
    "                                        new_answer[doc_id][context].append(second_phrase_index)\n",
    "                                        m += 1\n",
    "                                        n += 1\n",
    "                                    elif first_phrase_index < second_phrase_index:\n",
    "                                        m += 1\n",
    "                                    else:\n",
    "                                        n += 1\n",
    "                            except:\n",
    "                                continue\n",
    "                answer = new_answer                         \n",
    "                                        \n",
    "            else:\n",
    "                return answer\n",
    "                                            \n",
    "positional_intersect('persian gulf iran', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 persian gulf\n",
      "1 gulf iran\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['خلیج', 'فارس', 'ایر']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(1, 2), (3, 4)]\n",
    "\n",
    "for index, (i, j) in enumerate([('persian', 'gulf'), ('gulf', 'iran')]):\n",
    "    print(index, i, j)\n",
    "    \n",
    "a = [('persian', 'gulf'), ('gulf', 'iran')]\n",
    "for inde"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

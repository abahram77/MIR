{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:07.981141Z",
     "start_time": "2020-07-24T02:50:07.976322Z"
    }
   },
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import os\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:08.730650Z",
     "start_time": "2020-07-24T02:50:08.709430Z"
    }
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_stopwords(directory):\n",
    "    stopwords = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            for stopword in f.read().splitlines():\n",
    "                if stopword not in stopwords:\n",
    "                    stopwords.append(stopword)\n",
    "    return stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:09.420944Z",
     "start_time": "2020-07-24T02:50:09.385194Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [],
   "source": [
    "# def prepare_text(raw_text):\n",
    "#     prepared_text = []\n",
    "#     stopwords = get_stopwords('stopwords')\n",
    "    \n",
    "#     normalizer = Normalizer()\n",
    "#     normalized_text = normalizer.normalize(raw_text)\n",
    "#     for stop in stopwords:\n",
    "#         normalized_text = normalized_text.replace(stop, \" \")\n",
    "#     tokenized_text = word_tokenize(normalized_text)\n",
    "#     stemmer = Stemmer()\n",
    "    \n",
    "#     for token in tokenized_text:\n",
    "#         if token not in stopwords:\n",
    "#             stemmed_text = stemmer.stem(token)\n",
    "#             if stemmed_text != \"\":\n",
    "#                 prepared_text.append(stemmed_text)\n",
    "\n",
    "#     return prepared_text\n",
    "\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = []\n",
    "    normalizer = Normalizer()\n",
    "    normalized_text = normalizer.normalize(raw_text)\n",
    "    tokenized_text = word_tokenize(normalized_text)\n",
    "    stopwords = get_stopwords('stopwords')\n",
    "    stemmer = Stemmer()\n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        if token not in stopwords:\n",
    "            stemmed_text = stemmer.stem(token)\n",
    "            if stemmed_text != \"\":\n",
    "                prepared_text.append(stemmed_text)\n",
    "\n",
    "    return prepared_text\n",
    "\n",
    "# print('Enter text:')\n",
    "# raw_text = input()\n",
    "# print(prepare_text(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " < d (30 نمره)   iv style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:10.780397Z",
     "start_time": "2020-07-24T02:50:10.775854Z"
    }
   },
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:11.092665Z",
     "start_time": "2020-07-24T02:50:11.085829Z"
    }
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_title(raw_title):\n",
    "    title = raw_title[0].childNodes[0].data\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:11.352344Z",
     "start_time": "2020-07-24T02:50:11.348373Z"
    }
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_text(raw_text):\n",
    "    text = raw_text[0].childNodes[0].data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:11.584035Z",
     "start_time": "2020-07-24T02:50:11.581102Z"
    }
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_id(raw_id):\n",
    "    doc_id = raw_id[0].childNodes[0].data\n",
    "    return doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:43.865683Z",
     "start_time": "2020-07-24T02:50:11.874850Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "positional_index = {}\n",
    "doc_length = {} #number of words for each documents\n",
    "normalization = {}\n",
    "\n",
    "def document_length(title_words_list, text_words_list, doc_id):\n",
    "    all_words_freq = {}\n",
    "    all_words = []\n",
    "\n",
    "\n",
    "    for word in title_words_list:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "            all_words_freq[word] = 1\n",
    "        else:\n",
    "            all_words_freq[word] += 1\n",
    "    \n",
    "    for word in text_words_list:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "            all_words_freq[word] = 1\n",
    "        else:\n",
    "            all_words_freq[word] += 1\n",
    "            \n",
    "    normal = 0        \n",
    "    for word in all_words_freq:\n",
    "        frequency = all_words_freq[word]\n",
    "        weight = 1 + math.log(frequency)\n",
    "        normal += weight * weight\n",
    "            \n",
    "    normalization[doc_id] = normal\n",
    "    doc_length[doc_id] = len(all_words)\n",
    "    \n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    dom = parse(docs_path)\n",
    "    pages = dom.getElementsByTagName('page')    \n",
    "    for page in pages:\n",
    "        title = get_title(page.getElementsByTagName('title'))\n",
    "        text = get_text(page.getElementsByTagName('text'))\n",
    "        doc_id = get_id(page.getElementsByTagName('id'))\n",
    "        \n",
    "        text_words_list = prepare_text(text)\n",
    "        title_words_list = prepare_text(title)\n",
    "        \n",
    "        document_length(title_words_list, text_words_list, doc_id)\n",
    "        \n",
    "        for index, text_word in enumerate(text_words_list):\n",
    "            if text_word not in positional_index:\n",
    "                initializer = {doc_id: {'text': []}}\n",
    "                positional_index[text_word] = initializer\n",
    "            elif doc_id not in positional_index[text_word]:\n",
    "                positional_index[text_word][doc_id] = {'text': []}\n",
    "            elif 'text' not in positional_index[text_word][doc_id]:\n",
    "                positional_index[text_word][doc_id]['text'] = []\n",
    "            positional_index[text_word][doc_id]['text'].append(index)\n",
    "\n",
    "            \n",
    "        for index, title_word in enumerate(title_words_list):\n",
    "            if title_word not in positional_index:\n",
    "                initializer = {doc_id: {'title': []}}\n",
    "                positional_index[title_word] = initializer\n",
    "            elif doc_id not in positional_index[title_word]:\n",
    "                positional_index[title_word][doc_id] = {'title': []}\n",
    "            elif 'title' not in positional_index[title_word][doc_id]:\n",
    "                positional_index[title_word][doc_id]['title'] = []\n",
    "            positional_index[title_word][doc_id]['title'].append(index)\n",
    "    \n",
    "\n",
    "construct_positional_indexes('project1_data/data/Persian.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:43.870574Z",
     "start_time": "2020-07-24T02:50:43.867337Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_posting_list(word):\n",
    "    return positional_index[word]\n",
    "\n",
    "# get_posting_list('سلام')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:43.885945Z",
     "start_time": "2020-07-24T02:50:43.875502Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "\n",
    "# def get_words_with_bigram(bigram):\n",
    "#     # WARNING: not based on slides     \n",
    "#     words = []\n",
    "#     for word in positional_index:\n",
    "#         if bigram in word:\n",
    "#             words.append(word)\n",
    "#     return words\n",
    "\n",
    "def get_words_with_bigram(bigram):\n",
    "    # WARNING: based on slides    \n",
    "    inverted_index = {}\n",
    "    for word in positional_index:\n",
    "        word_length = len(word)\n",
    "        if word_length >= 2:\n",
    "            for i in range(len(word) - 2):\n",
    "                bi = word[i: i + 2]\n",
    "                if bi not in inverted_index:\n",
    "                    inverted_index[bi] = []\n",
    "                if word not in inverted_index[bi]:\n",
    "                    inverted_index[bi].append(word)\n",
    "    return inverted_index[bigram]\n",
    "\n",
    "# get_words_'Enter text:')\n",
    "# raw_text = input()\n",
    "# print(prepare_text(raw_text))with_bigram('لا')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:50:43.895622Z",
     "start_time": "2020-07-24T02:50:43.888893Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "#Done\n",
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    address = docs_path + '/' + doc_num + '.xml'\n",
    "    construct_positional_indexes(address)\n",
    "\n",
    "# add_document_to_indexes('project1_data/data/', 'add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:51:21.568055Z",
     "start_time": "2020-07-24T02:51:21.559649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3014': {'text': [14, 23, 33, 67], 'title': [1]},\n",
       " '3103': {'text': [2597]},\n",
       " '3119': {'text': [718]},\n",
       " '3197': {'text': [8212]},\n",
       " '3217': {'text': [814, 2476]},\n",
       " '3229': {'text': [582]},\n",
       " '3359': {'text': [1676, 1709]},\n",
       " '3373': {'text': [1002, 2258]},\n",
       " '3404': {'text': [236]},\n",
       " '3429': {'text': [154]},\n",
       " '3654': {'text': [2882, 3540]},\n",
       " '3699': {'text': [1800]},\n",
       " '3776': {'text': [1506]},\n",
       " '3777': {'text': [2404]},\n",
       " '3798': {'text': [352]},\n",
       " '3826': {'text': [926, 2322, 2661]},\n",
       " '3879': {'text': [1543]},\n",
       " '3897': {'text': [1464, 1494, 1509, 1514, 1521]},\n",
       " '3938': {'text': [675, 690, 2266, 2683, 3262, 3426, 3542, 3928, 4036, 4111]},\n",
       " '4002': {'text': [1433, 2366]},\n",
       " '4062': {'text': [259]},\n",
       " '4248': {'text': [439]},\n",
       " '4275': {'text': [543]},\n",
       " '4321': {'text': [1635, 3123]},\n",
       " '4335': {'text': [7096]},\n",
       " '4382': {'text': [146]},\n",
       " '4388': {'text': [1274]},\n",
       " '4391': {'text': [2514]},\n",
       " '4398': {'text': [2221, 2306, 6017]},\n",
       " '4400': {'text': [1749]},\n",
       " '4460': {'text': [2152, 3054]},\n",
       " '4636': {'text': [392]},\n",
       " '4650': {'text': [204]},\n",
       " '4671': {'text': [3547]},\n",
       " '4718': {'text': [163,\n",
       "   193,\n",
       "   1169,\n",
       "   2177,\n",
       "   3049,\n",
       "   3493,\n",
       "   4245,\n",
       "   4985,\n",
       "   6510,\n",
       "   7041,\n",
       "   7210,\n",
       "   7415,\n",
       "   7758,\n",
       "   7810,\n",
       "   7897,\n",
       "   7951,\n",
       "   9882,\n",
       "   11040,\n",
       "   12993,\n",
       "   13464,\n",
       "   13582,\n",
       "   13637,\n",
       "   13986]},\n",
       " '4743': {'text': [1512]},\n",
       " '4805': {'text': [515]},\n",
       " '4843': {'text': [2580, 3386]},\n",
       " '4864': {'text': [5518, 7881, 8155]},\n",
       " '5192': {'text': [7781]},\n",
       " '5308': {'text': [1914]},\n",
       " '5381': {'text': [1427]},\n",
       " '5486': {'text': [375, 437, 538, 1291, 2652]},\n",
       " '5554': {'text': [3809]},\n",
       " '5571': {'text': [17]},\n",
       " '5707': {'text': [764, 824]},\n",
       " '5720': {'text': [297]},\n",
       " '5820': {'text': [267]},\n",
       " '5967': {'text': [14242]},\n",
       " '6014': {'text': [134]},\n",
       " '6052': {'text': [394]},\n",
       " '6088': {'text': [160]},\n",
       " '6229': {'text': [21]},\n",
       " '6294': {'text': [1993]},\n",
       " '6417': {'text': [2313, 2581]},\n",
       " '6418': {'text': [1568, 10503, 13289]},\n",
       " '6475': {'text': [4164]},\n",
       " '6522': {'text': [1606]},\n",
       " '6568': {'text': [835]},\n",
       " '6572': {'text': [197]},\n",
       " '6609': {'text': [522]},\n",
       " '6629': {'text': [2108]},\n",
       " '6634': {'text': [183, 358, 369, 1150]},\n",
       " '6710': {'text': [641]},\n",
       " '6735': {'text': [7011]},\n",
       " '6749': {'text': [3850, 3854, 3860, 3964]},\n",
       " '6752': {'text': [2399]},\n",
       " '6753': {'text': [7966]},\n",
       " '6791': {'text': [370, 650, 2923]},\n",
       " '6847': {'text': [1579]},\n",
       " '6848': {'text': [634]},\n",
       " '6907': {'text': [1129, 1144, 1380, 1385, 2751, 2757, 2775, 2794]},\n",
       " '6931': {'text': [1069]},\n",
       " '6944': {'text': [867, 1399, 2841]},\n",
       " '6959': {'text': [652, 1972]},\n",
       " '6973': {'text': [3143, 3261, 3270, 3659, 4627, 5464]},\n",
       " '7133': {'text': [595]}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_index['فکر']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T02:51:37.815521Z",
     "start_time": "2020-07-24T02:51:37.647991Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کلمه‌ی '''بازی در سند 3014 پیدا شد\n",
      "کلمه‌ی فکری''' در سند 3014 پیدا شد\n",
      "کلمه‌ی از در سند 3014 پیدا شد\n",
      "کلمه‌ی جمله در سند 3014 پیدا شد\n",
      "کلمه‌ی بازی در سند 3014 پیدا شد\n",
      "کلمه‌ی اس در سند 3014 پیدا شد\n",
      "کلمه‌ی که در سند 3014 پیدا شد\n",
      "کلمه‌ی نا در سند 3014 پیدا شد\n",
      "کلمه‌ی آن در سند 3014 پیدا شد\n",
      "کلمه‌ی پیدا در سند 3014 پیدا شد\n",
      "کلمه‌ی بر در سند 3014 پیدا شد\n",
      "کلمه‌ی اساس در سند 3014 پیدا شد\n",
      "کلمه‌ی فکر در سند 3014 پیدا شد\n",
      "کلمه‌ی و در سند 3014 پیدا شد\n",
      "کلمه‌ی تمرکز در سند 3014 پیدا شد\n",
      "کلمه‌ی انس در سند 3014 پیدا شد\n",
      "کلمه‌ی انجا در سند 3014 پیدا شد\n",
      "کلمه‌ی می‌گیرد در سند 3014 پیدا شد\n",
      "کلمه‌ی می‌تو در سند 3014 پیدا شد\n",
      "کلمه‌ی به در سند 3014 پیدا شد\n",
      "کلمه‌ی موارد در سند 3014 پیدا شد\n",
      "کلمه‌ی زیر در سند 3014 پیدا شد\n",
      "کلمه‌ی اشاره در سند 3014 پیدا شد\n",
      "کلمه‌ی کرد در سند 3014 پیدا شد\n",
      "کلمه‌ی پنتاگو در سند 3014 پیدا شد\n",
      "کلمه‌ی شطرنج در سند 3014 پیدا شد\n",
      "کلمه‌ی نرد در سند 3014 پیدا شد\n",
      "کلمه‌ی بکر در سند 3014 پیدا شد\n",
      "کلمه‌ی دیدار در سند 3014 پیدا شد\n",
      "کلمه‌ی قاتل در سند 3014 پیدا شد\n",
      "کلمه‌ی هشت‌پا در سند 3014 پیدا شد\n",
      "کلمه‌ی ورق‌باز در سند 3014 پیدا شد\n",
      "کلمه‌ی بیس در سند 3014 پیدا شد\n",
      "کلمه‌ی سوال در سند 3014 پیدا شد\n",
      "کلمه‌ی نقطه در سند 3014 پیدا شد\n",
      "کلمه‌ی باز در سند 3014 پیدا شد\n",
      "کلمه‌ی مافیا در سند 3014 پیدا شد\n",
      "کلمه‌ی گروه در سند 3014 پیدا شد\n",
      "کلمه‌ی سودوکو در سند 3014 پیدا شد\n",
      "کلمه‌ی جدول در سند 3014 پیدا شد\n",
      "کلمه‌ی کل در سند 3014 پیدا شد\n",
      "کلمه‌ی متقاطع در سند 3014 پیدا شد\n",
      "کلمه‌ی مکعب در سند 3014 پیدا شد\n",
      "کلمه‌ی روبیک در سند 3014 پیدا شد\n",
      "کلمه‌ی چامپ در سند 3014 پیدا شد\n",
      "کلمه‌ی تخته در سند 3014 پیدا شد\n",
      "کلمه‌ی ویدیو در سند 3014 پیدا شد\n",
      "کلمه‌ی رایانه در سند 3014 پیدا شد\n",
      "کلمه‌ی آتار در سند 3014 پیدا شد\n",
      "کلمه‌ی پانویس در سند 3014 پیدا شد\n",
      "کلمه‌ی خانگ در سند 3014 پیدا شد\n",
      "کلمه‌ی بازی-خرد در سند 3014 پیدا شد\n",
      "کلمه‌ی سرگرمی-خرد در سند 3014 پیدا شد\n",
      "کلمه‌ی رده در سند 3014 پیدا شد\n",
      "کلمه‌ی مهارت در سند 3014 پیدا شد\n"
     ]
    }
   ],
   "source": [
    "#Done\n",
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    # docs_path should be the address of the document contains deletetion words or docs\n",
    "#     address = docs_path\n",
    "#     dom = parse(address)\n",
    "#     pages = dom.getElementsByTagName('page')    \n",
    "#     for page in pages:\n",
    "#         title = get_title(page.getElementsByTagName('title'))\n",
    "#         text = get_text(page.getElementsByTagName('text'))\n",
    "#         doc_id = get_id(page.getElementsByTagName('id'))\n",
    "\n",
    "#         text_words_list = prepare_text(text)\n",
    "#         title_words_list = prepare_text(title)\n",
    "    if type(doc_num) == type(1):\n",
    "        doc_num = str(doc_num)\n",
    "    \n",
    "    positional_index_copy = positional_index.copy()\n",
    "    for index, word in enumerate(positional_index_copy):\n",
    "        if doc_num in positional_index_copy[word]:\n",
    "            print('کلمه‌ی', word, 'در سند', doc_num, 'پیدا شد')\n",
    "            del positional_index[word][doc_num]\n",
    "        try:\n",
    "            if len(positional_index[word]) == 0:\n",
    "                del positional_index[word]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "delete_document_from_indexes('project1_data/data/Persian.xml', '3014')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:05:13.384859Z",
     "start_time": "2020-07-23T14:05:13.373521Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    full_destination = destination + '.json'\n",
    "    with open(full_destination, 'w') as f:\n",
    "        json.dump(positional_index, f, ensure_ascii=False)\n",
    "\n",
    "# save_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:05:16.238147Z",
     "start_time": "2020-07-23T14:05:16.214214Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    full_destination = source + '.json'\n",
    "    with open(full_destination, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "\n",
    "# load_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [],
   "source": [
    "def correct_query(query):\n",
    "    correct_query = \"سلام حالا پرسمان درست شد.\"\n",
    "    return correct_query\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:05:49.825480Z",
     "start_time": "2020-07-23T14:05:49.686634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Done\n",
    "def positional_intersect(phrase):\n",
    "    phrases = prepare_text(phrase)\n",
    "    if len(phrases) == 1:\n",
    "        return get_posting_list(phrases[0])\n",
    "    \n",
    "    phrase_touples = []\n",
    "    \n",
    "    for i in range(len(phrases) - 1):\n",
    "        phrase_touples.append((phrases[i], phrases[i + 1]))\n",
    "    \n",
    "    answer = {}\n",
    "    \n",
    "    for index, (phrase1, phrase2) in enumerate(phrase_touples):\n",
    "        doc_list1 = {}\n",
    "        if index == 0:\n",
    "            doc_list1 = positional_index[phrase1]\n",
    "            \n",
    "        elif not answer:\n",
    "            return answer\n",
    "        \n",
    "        else:\n",
    "            doc_list1 = answer\n",
    "            answer = {}\n",
    "\n",
    "        try:\n",
    "            doc_list_len1 = len(doc_list1)\n",
    "            doc_list2 = positional_index[phrase2]\n",
    "            doc_list_len2 = len(doc_list2)\n",
    "\n",
    "        except:\n",
    "            return answer\n",
    "        i, j = 0, 0\n",
    "        while i < doc_list_len1 and j < doc_list_len2:\n",
    "            doc1_keys = list(doc_list1.keys())\n",
    "            doc2_keys = list(doc_list2.keys())\n",
    "\n",
    "\n",
    "            doc1_key = doc1_keys[i]\n",
    "            doc2_key = doc2_keys[j]\n",
    "\n",
    "\n",
    "            if doc1_key == doc2_key:\n",
    "                doc_id = doc1_key\n",
    "                contexts = ['title', 'text']\n",
    "                for context in contexts:\n",
    "                    try:\n",
    "                        context_indices1 = doc_list1[doc1_key][context]\n",
    "                        context_indices2 = doc_list2[doc2_key][context]\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    m, n = 0, 0\n",
    "                    while m < len(context_indices1) and n < len(context_indices2):\n",
    "                        first_phrase_index = context_indices1[m]\n",
    "                        second_phrase_index = context_indices2[n]\n",
    "\n",
    "                        if second_phrase_index - first_phrase_index == 1:\n",
    "                            if doc_id not in answer:\n",
    "                                answer[doc_id] = {}\n",
    "\n",
    "                            if context not in answer[doc_id]:\n",
    "                                answer[doc_id][context] = []\n",
    "\n",
    "                            answer[doc_id][context].append(second_phrase_index)\n",
    "\n",
    "                            m += 1\n",
    "                            n += 1\n",
    "                        elif first_phrase_index < second_phrase_index:\n",
    "                            m += 1\n",
    "                        else:\n",
    "                            n += 1\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "\n",
    "            elif doc1_key < doc2_key:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "                \n",
    "#     print(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:05:53.025115Z",
     "start_time": "2020-07-23T14:05:52.961721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Done\n",
    "def phrase_normalizer(double_quoted):\n",
    "    new_double_quotes = []\n",
    "    for quote in double_quoted:\n",
    "        quote_list = prepare_text(quote)\n",
    "        phrase = \"\"\n",
    "        for quotes in quote_list:\n",
    "            phrase += quotes + \" \"\n",
    "        new_double_quotes.append(phrase.strip())\n",
    "    return new_double_quotes\n",
    "\n",
    "def parsing_query(query):\n",
    "    all_quotations = re.findall('\"([^\"]*)\"', query)\n",
    "    query = query.replace('\"', \"\")\n",
    "    for quoted in all_quotations:\n",
    "        query = query.replace(quoted, \"\")\n",
    "        query = query.replace('  ', \" \")\n",
    "    normalized_phrases = phrase_normalizer(all_quotations[:])\n",
    "    not_sequential = prepare_text(query)\n",
    "    return normalized_phrases, not_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:06:08.655539Z",
     "start_time": "2020-07-23T14:06:08.478615Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [],
   "source": [
    "# Done \n",
    "N = len(doc_length)\n",
    "\n",
    "def get_quoted_posting_list(phrases):\n",
    "    first_double_quote = phrases.pop()\n",
    "    first_double_quote_info = positional_intersect(first_double_quote)\n",
    "    first_double_quote_docs = list(first_double_quote_info.keys()) #docs containing the first double quote\n",
    "    answer = first_double_quote_docs\n",
    "    \n",
    "    for double_quote in phrases:\n",
    "        double_quote_info = positional_intersect(double_quote)\n",
    "        if double_quote_info:            \n",
    "            other_double_quote_docs = list(double_quote_info.keys())\n",
    "            new_answer = []\n",
    "            for ans_doc_id in answer:\n",
    "                for doc_id_other in other_double_quote_docs:\n",
    "                    if ans_doc_id == doc_id_other:\n",
    "                        new_answer.append(ans_doc_id)\n",
    "            answer = new_answer\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def get_dictionary(query_list):\n",
    "    dic = {}\n",
    "    for term in query_list:\n",
    "        if term not in dic:\n",
    "            dic[term] = 1\n",
    "        else:\n",
    "            dic[term] += 1\n",
    "    return dic\n",
    "\n",
    "\n",
    "def consine_score(query_list, method, weight, quoted=False, double_quoted=[], contexts=['title', 'text']):\n",
    "    if quoted: #new\n",
    "        valid_docs = get_quoted_posting_list(double_quoted[:]) #docs that have all phrases\n",
    "        \n",
    "    else:\n",
    "        valid_docs = list(doc_length.keys())\n",
    "        \n",
    "    scores = {} #score of documents\n",
    "    query_terms_occurence = get_dictionary(query_list) #terms occurence in query\n",
    "    terms = list(query_terms_occurence.keys()) #all distinct terms in query\n",
    "    doc_normalization = 0\n",
    "    \n",
    "#     print('terms', terms)\n",
    "\n",
    "    for term in terms:\n",
    "        if term in positional_index or term in double_quoted:\n",
    "            if quoted:\n",
    "                docs_list = positional_intersect(term)\n",
    "            else:\n",
    "                docs_list = positional_index[term]\n",
    "\n",
    "            tf_q = query_terms_occurence[term]\n",
    "            document_occurence = len(docs_list)\n",
    "#             print('term:', term, 'query occ:', query_terms_occurence[term], 'doc occ:', document_occurence)\n",
    "            \n",
    "            idf = math.log(N / document_occurence)\n",
    "            w_tq = tf_q * idf\n",
    "\n",
    "            for doc in docs_list:\n",
    "                if doc in valid_docs:\n",
    "                    tf = 0\n",
    "                    for context in contexts:\n",
    "                        if 'title' in docs_list[doc] and context == 'title':\n",
    "                            title_length = len(docs_list[doc]['title'])\n",
    "                            tf += weight * (1 + math.log(title_length))\n",
    "                        elif 'text' in docs_list[doc] and context == 'text':\n",
    "                            text_length = len(docs_list[doc]['text'])\n",
    "                            tf += 1 + math.log(text_length)    \n",
    "                        w_td = tf\n",
    "                        doc_normalization += w_td * w_td\n",
    "                        if doc in scores:\n",
    "                            scores[doc] += w_tq * w_td\n",
    "                        else:\n",
    "                            scores[doc] = w_tq * w_td\n",
    "    if method == 'ltc-lnc':\n",
    "        doc_normalization = math.sqrt(doc_normalization)\n",
    "#         print('doc_normalization', doc_normalization)\n",
    "        for doc in scores:\n",
    "            scores[doc] /= normalization[doc]\n",
    "    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_scores[:15]\n",
    "\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2, contexts=['title', 'text']):\n",
    "#     print('query', query)\n",
    "    double_quoted, not_sequential = parsing_query(query)\n",
    "\n",
    "    all_words = [] #new\n",
    "    all_words.extend(double_quoted + not_sequential) #new\n",
    "\n",
    "    if not double_quoted:\n",
    "        return consine_score(all_words[:], method, weight, contexts=contexts[:])\n",
    "    \n",
    "    return consine_score(all_words, method, weight, True, double_quoted[:], contexts=contexts[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:09:22.247992Z",
     "start_time": "2020-07-23T14:09:22.234739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3903', 0.14654504603815563), ('5724', 0.07923601100940733), ('3411', 0.01749701063093364), ('3415', 0.011040051865250899), ('3416', 0.010429442964682765), ('3414', 0.009752258601506105), ('6629', 0.007082597400591555), ('4259', 0.006831996492132716), ('6266', 0.006602654204992658), ('4376', 0.006423041594737472), ('4627', 0.005990729840625074), ('4659', 0.0044631364557670685), ('4624', 0.0009856208453320734)]\n"
     ]
    }
   ],
   "source": [
    "query = 'سیاره های بزرگ \"منظومه شمسی\"'\n",
    "method = \"ltc-lnc\"\n",
    "\n",
    "relevant_docs = search(query, method)\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:06:57.457227Z",
     "start_time": "2020-07-23T14:06:57.392249Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    relevant_docs = {}\n",
    "    title_score = search(title_query, method=method, weight=2, contexts=['title'])\n",
    "    text_score = search(text_query, method=method, weight=1, contexts=['text'])\n",
    "    for doc_id, score in title_score:\n",
    "        if doc_id in relevant_docs:\n",
    "            relevant_docs[doc_id] += score\n",
    "        else:\n",
    "            relevant_docs[doc_id] = score\n",
    "    \n",
    "    for doc_id, score in text_score:\n",
    "        if doc_id in relevant_docs:\n",
    "            relevant_docs[doc_id] += score\n",
    "        else:\n",
    "            relevant_docs[doc_id] = score\n",
    "            \n",
    "    scores = sorted(relevant_docs.items(), key=lambda item: item[1], reverse=True)\n",
    "    return scores[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:10:39.961260Z",
     "start_time": "2020-07-23T14:10:39.826227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3760', 37.02344832828203), ('5236', 28.27685697751118), ('5264', 21.212952260226878), ('3874', 17.576092973339307), ('4275', 16.35108896331902), ('6394', 16.18340665767392), ('4094', 16.105987788385384), ('5381', 15.268534610866807), ('4339', 13.878146186555009), ('6159', 13.839141263669335), ('4074', 11.711170998186402), ('4907', 11.66021627736957), ('6369', 11.646137703206685), ('6694', 11.407188476107676), ('6457', 11.155726436370928)]\n"
     ]
    }
   ],
   "source": [
    "title_query = 'فهرست شهرهای ایران'\n",
    "text_query = 'استان گیلان شهرستان لنگرود'\n",
    "##################################\n",
    "\n",
    "relevant_docs = detailed_search(title_query, text_query)\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:07:00.855361Z",
     "start_time": "2020-07-23T14:07:00.762789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Done\n",
    "def get_queries(query_id='all'):\n",
    "    directory = 'project1_data/data/'\n",
    "    queries = {}\n",
    "    relevances = {}\n",
    "    query_directory = directory + 'queries/'\n",
    "    relevant_directory = directory + 'relevance/'\n",
    "\n",
    "    if query_id == 'all':\n",
    "        for filename in os.listdir(query_directory):\n",
    "            with open (os.path.join(query_directory, filename), 'r') as f:\n",
    "                query = f.read()\n",
    "                queries[filename[:-4]] = query\n",
    "    \n",
    "        for filename in os.listdir(relevant_directory):\n",
    "            with open (os.path.join(relevant_directory, filename), 'r') as f:\n",
    "                relevance = f.read()\n",
    "                relevant_docs = relevance.split(', ')\n",
    "                relevances[filename[:-4]] = relevant_docs\n",
    "    else:\n",
    "        query_directory += query_id + '.txt'\n",
    "        with open (query_directory, 'r') as f:\n",
    "            query = f.read()\n",
    "            queries[query_id] = query\n",
    "        \n",
    "        relevant_directory += query_id + '.txt'\n",
    "        with open (relevant_directory, 'r') as f:\n",
    "            relevance = f.read()\n",
    "            relevant_docs = relevance.split(', ')\n",
    "            relevances[query_id] = relevant_docs\n",
    "    return queries, relevances\n",
    "\n",
    "queries, relevants = get_queries(query_id='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:07:02.554625Z",
     "start_time": "2020-07-23T14:07:02.049411Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5905299552024381\n"
     ]
    }
   ],
   "source": [
    "def R_Precision(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        func_queries = queries\n",
    "        func_relevants = relevants\n",
    "        keys = list(func_queries.keys())\n",
    "        query_no = len(keys)\n",
    "\n",
    "    else:\n",
    "        func_queries = queries[query_id]\n",
    "        func_relevants = relevants[query_id]\n",
    "        keys = [query_id]\n",
    "        query_no = 1\n",
    "        \n",
    "    final_result = 0\n",
    "        \n",
    "    for key in keys:\n",
    "        if '\\n' in func_queries[key]:\n",
    "            title_query, text_query = func_queries[key].split('\\n')\n",
    "            related = detailed_search(title_query, text_query)\n",
    "        else:\n",
    "            related = search(func_queries[key])\n",
    "            \n",
    "        tp_list = []\n",
    "        for doc_id, score in related:\n",
    "            if doc_id in func_relevants[key]:\n",
    "                tp_list.append(doc_id)\n",
    "                \n",
    "        tp = len(tp_list)\n",
    "           \n",
    "#         print()\n",
    "#         print('func_relevants[key]', sorted(func_relevants[key]))\n",
    "#         print('tp:', tp)\n",
    "#         print('relevance', len(func_relevants[key]))\n",
    "#         print()\n",
    "        \n",
    "        final_result +=  tp / (len(func_relevants[key]))\n",
    "        \n",
    "    return final_result / query_no\n",
    "\n",
    "print(R_Precision())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:07:04.488645Z",
     "start_time": "2020-07-23T14:07:03.962556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5459785128304053\n"
     ]
    }
   ],
   "source": [
    "# Done\n",
    "def F_measure(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        func_queries = queries\n",
    "        func_relevants = relevants\n",
    "        keys = list(func_queries.keys())\n",
    "        query_no = len(queries)\n",
    "\n",
    "    else:\n",
    "        func_queries = queries[query_id]\n",
    "        func_relevants = relevants[query_id]\n",
    "        keys = [query_id]\n",
    "        query_no = 1\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    for key in keys:\n",
    "        if '\\n' in func_queries[key]:\n",
    "            title_query, text_query = func_queries[key].split('\\n')\n",
    "            related = detailed_search(title_query, text_query)\n",
    "        else:\n",
    "            related = search(func_queries[key])\n",
    "        \n",
    "        tp_list = []\n",
    "        for doc_id, score in related:\n",
    "            if doc_id in func_relevants[key]:\n",
    "                tp_list.append(doc_id)\n",
    "                \n",
    "        tp = len(tp_list)\n",
    "        p = tp / len(related)\n",
    "        r = tp / len(func_relevants[key])\n",
    "        \n",
    "        if p + r != 0:\n",
    "            result = result + (2 * p * r)/(p + r)\n",
    "    \n",
    "    final_result = result / query_no\n",
    "    return final_result\n",
    "\n",
    "print(F_measure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:07:06.426541Z",
     "start_time": "2020-07-23T14:07:05.882501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7250292822946315\n"
     ]
    }
   ],
   "source": [
    "# Done\n",
    "def MAP(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        func_queries = queries\n",
    "        func_relevants = relevants\n",
    "        keys = list(func_queries.keys())\n",
    "    else:\n",
    "        func_queries = queries[query_id]\n",
    "        func_relevants = relevants[query_id]\n",
    "        keys = [query_id]\n",
    "    \n",
    "    ap = 0\n",
    "    \n",
    "    for key in keys:\n",
    "        if '\\n' in func_queries[key]:\n",
    "            title_query, text_query = func_queries[key].split('\\n')\n",
    "            related = detailed_search(title_query, text_query)\n",
    "        else:\n",
    "            related = search(func_queries[key])\n",
    "            \n",
    "        p_at_k_sum, tp = 0, 0\n",
    "        for i, (doc_id, score) in enumerate(related):\n",
    "            if doc_id in func_relevants[key]:\n",
    "                tp += 1\n",
    "                p_at_k_sum += tp / (i + 1)\n",
    "                \n",
    "        if tp != 0:\n",
    "            ap += p_at_k_sum / tp\n",
    "    return ap / len(keys)\n",
    "\n",
    "print(MAP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T14:07:08.077478Z",
     "start_time": "2020-07-23T14:07:07.570811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5080581613902706\n"
     ]
    }
   ],
   "source": [
    "def NDCG(query_id='all'):\n",
    "    if query_id == 'all':\n",
    "        func_queries = queries\n",
    "        func_relevants = relevants\n",
    "        keys = list(func_queries.keys())\n",
    "        query_no = len(queries)\n",
    "    else:\n",
    "        func_queries = queries[query_id]\n",
    "        func_relevants = relevants[query_id]\n",
    "        keys = [query_id]\n",
    "        query_no= 1\n",
    "    \n",
    "    ndcg = 0\n",
    "    for key in keys:\n",
    "        if '\\n' in func_queries[key]:\n",
    "            title_query, text_query = func_queries[key].split('\\n')\n",
    "            related = detailed_search(title_query, text_query)\n",
    "#             print('related', related)\n",
    "#             print('key', key)\n",
    "        else:\n",
    "            related = search(func_queries[key])\n",
    "        main = 1\n",
    "        \n",
    "        for i, doc_id in enumerate(func_relevants[key]):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            main += 1 / math.log(i + 2)\n",
    "\n",
    "        for i, (doc_id, score) in enumerate(related):\n",
    "            \n",
    "            if i == 0:\n",
    "                if doc_id in related:\n",
    "                    dcg = 1\n",
    "                else:\n",
    "                    dcg = 0\n",
    "                continue\n",
    "            if doc_id in func_relevants[key]:\n",
    "                dcg += 1 / math.log(i + 2)\n",
    "                \n",
    "        ndcg += dcg / main\n",
    "        \n",
    "    return ndcg / query_no\n",
    "\n",
    "print(NDCG())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_stopwords(directory):\n",
    "    stopwords = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r') as f:\n",
    "            for stopword in f.read().splitlines():\n",
    "                if stopword not in stopwords:\n",
    "                    stopwords.append(stopword)\n",
    "    return stopwords        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = []\n",
    "    normalizer = Normalizer()\n",
    "    normalized_text = normalizer.normalize(raw_text)\n",
    "    tokenized_text = word_tokenize(normalized_text)\n",
    "    stopwords = get_stopwords('stopwords')\n",
    "    stemmer = Stemmer()\n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        if token not in stopwords:\n",
    "            stemmed_text = stemmer.stem(token)\n",
    "            if stemmed_text != \"\":\n",
    "                \n",
    "                prepared_text.append(stemmed_text.strip())\n",
    "\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = []\n",
    "    stopwords = get_stopwords('stopwords')\n",
    "    \n",
    "    normalizer = Normalizer()\n",
    "    normalized_text = normalizer.normalize(raw_text)\n",
    "    for stop in stopwords:\n",
    "        normalized_text = normalized_text.replace(stop, \" \")\n",
    "    tokenized_text = word_tokenize(normalized_text)\n",
    "    stemmer = Stemmer()\n",
    "    \n",
    "    for token in tokenized_text:\n",
    "        if token not in stopwords:\n",
    "            stemmed_text = stemmer.stem(token)\n",
    "            if stemmed_text != \"\":\n",
    "                prepared_text.append(stemmed_text)\n",
    "\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "from xml.dom.minidom import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "# Reading Data (XML file) and store it in dom variable\n",
    "dom = parse('project1_data/data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_title(raw_title):\n",
    "    title = raw_title[0].childNodes[0].data\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_text(raw_text):\n",
    "    text = raw_text[0].childNodes[0].data\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_id(raw_id):\n",
    "    doc_id = raw_id[0].childNodes[0].data\n",
    "    return doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "positional_index = {}\n",
    "doc_length = {} #number of words for each documents\n",
    "\n",
    "def document_length(title_words_list, text_words_list, doc_id):\n",
    "    all_words = []\n",
    "    for word in title_words_list:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    for word in text_words_list:\n",
    "        if word not in all_words:\n",
    "            all_words.append(word)\n",
    "    doc_length[doc_id] = len(all_words)\n",
    "    \n",
    "\n",
    "def construct_positional_indexes(docs_path):\n",
    "    dom = parse(docs_path)\n",
    "    pages = dom.getElementsByTagName('page')    \n",
    "    for page in pages:\n",
    "        title = get_title(page.getElementsByTagName('title'))\n",
    "        text = get_text(page.getElementsByTagName('text'))\n",
    "        doc_id = get_id(page.getElementsByTagName('id'))\n",
    "        \n",
    "        text_words_list = prepare_text(text)\n",
    "        title_words_list = prepare_text(title)\n",
    "        \n",
    "        document_length(title_words_list, text_words_list, doc_id)\n",
    "        \n",
    "        for index, text_word in enumerate(text_words_list):\n",
    "            if text_word not in positional_index:\n",
    "                initializer = {doc_id: {'text': []}}\n",
    "                positional_index[text_word] = initializer\n",
    "            elif doc_id not in positional_index[text_word]:\n",
    "                positional_index[text_word][doc_id] = {'text': []}\n",
    "            elif 'text' not in positional_index[text_word][doc_id]:\n",
    "                positional_index[text_word][doc_id]['text'] = []\n",
    "            positional_index[text_word][doc_id]['text'].append(index)\n",
    "\n",
    "            \n",
    "        for index, title_word in enumerate(title_words_list):\n",
    "            if title_word not in positional_index:\n",
    "                initializer = {doc_id: {'title': []}}\n",
    "                positional_index[title_word] = initializer\n",
    "            elif doc_id not in positional_index[title_word]:\n",
    "                positional_index[title_word][doc_id] = {'title': []}\n",
    "            elif 'title' not in positional_index[title_word][doc_id]:\n",
    "                positional_index[title_word][doc_id]['title'] = []\n",
    "            positional_index[title_word][doc_id]['title'].append(index)\n",
    "    \n",
    "\n",
    "construct_positional_indexes('project1_data/data/Persian.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done\n",
    "def get_posting_list(word):\n",
    "    # Make sure how to get posting list! From reading file or as an argument\n",
    "    return positional_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    # WARNING: not based on slides     \n",
    "    words = []\n",
    "    for word in positional_index:\n",
    "        if bigram in word:\n",
    "            words.append(word)\n",
    "    return words\n",
    "# get_words_with_bigram('لا')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    # WARNING: based on slides    \n",
    "    inverted_index = {}\n",
    "    for word in positional_index:\n",
    "        word_length = len(word)\n",
    "        if word_length >= 2:\n",
    "            for i in range(len(word) - 2):\n",
    "                bi = word[i: i + 2]\n",
    "                if bi not in inverted_index:\n",
    "                    inverted_index[bi] = []\n",
    "                if word not in inverted_index[bi]:\n",
    "                    inverted_index[bi].append(word)\n",
    "    return inverted_index[bigram]\n",
    "                    \n",
    "# get_words_with_bigram('لا')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    address = docs_path + '/' + doc_num + '.xml'\n",
    "    construct_positional_indexes(address)\n",
    "\n",
    "# add_document_to_indexes('data/wiki', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    address = docs_path + '/' + doc_num + '.xml'\n",
    "    dom = parse(address)\n",
    "    pages = dom.getElementsByTagName('page')    \n",
    "    for page in pages:\n",
    "        title = get_title(page.getElementsByTagName('title'))\n",
    "        text = get_text(page.getElementsByTagName('text'))\n",
    "        doc_id = get_id(page.getElementsByTagName('id'))\n",
    "        \n",
    "        text_words_list = prepare_text(text)\n",
    "        title_words_list = prepare_text(title)\n",
    "        \n",
    "        for index, text_word in enumerate(text_words_list):\n",
    "            if text_word not in positional_index:\n",
    "                print('کلمه پیدا نشد')\n",
    "            elif doc_id not in positional_index[text_word]:\n",
    "                print('چنین کلمه‌ای با این شماره سند موجود نیست')\n",
    "            else:\n",
    "                positional_index[text_word].remove(doc_id)\n",
    "            if len(positional_index[text_word]) == 0:\n",
    "                del positional_index[text_word]\n",
    "                \n",
    "        for index, title_word in enumerate(title_words_list):\n",
    "            if title_word not in positional_index:\n",
    "                print('کلمه پیدا نشد')\n",
    "            elif doc_id not in positional_index[title_word]:\n",
    "                print('چنین کلمه‌ای با این شماره سند موجود نیست')\n",
    "            else:\n",
    "                positional_index[title_word].remove(doc_id)\n",
    "            if len(positional_index[title_word]) == 0:\n",
    "                del positional_index[title_word]\n",
    "\n",
    "# delete_document_from_indexes('data/wiki', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    full_destination = destination + '.json'\n",
    "    with open(full_destination, 'w') as f:\n",
    "        json.dump(positional_index, f, ensure_ascii=False)\n",
    "\n",
    "# save_index('storage/index_backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['طبیعت', 'دامنه', 'کوه', 'ایرانی'])\n",
      "(['علوم اجتماعی'], ['مطالعه', 'در', 'دانشگاه'])\n",
      "([], ['هیتلر', 'در', 'جنگ', 'جهانی', 'اول'])\n",
      "(['منظومه شمسی'], ['سیاره', 'های', 'بزرگ', '\\n'])\n",
      "([], ['جنگل', 'های', 'بلوط', 'ایران'])\n",
      "([], ['زندگی', 'حیوانات', 'وحشی'])\n",
      "([], ['مسابقات', 'فوتبال', 'المپیک'])\n",
      "([], ['کشورهای', 'عضو', 'اتحادیه', 'آفریقا'])\n",
      "([], ['کتاب', 'های', 'برگزیده', 'کودک', 'و', 'نوجوان'])\n",
      "([], ['برنده', 'جایزه', 'بهترین', 'فیلم', 'در', 'جشنواره'])\n",
      "([], ['ابزار', 'های', 'فضایی', 'و', 'پیشرفته', 'ناسا'])\n",
      "([], ['سواحل', 'دریای', 'سرخ'])\n",
      "(['خلیج فارس'], ['پایتخت', 'کشورهای', 'حوزه', ''])\n",
      "([], ['کشورهای', 'دارای', 'نفت', 'در', 'خاورمینا'])\n",
      "([], ['انتخابات', 'نمایندگان', 'مجلس', 'ایالتی', 'در', 'آمریکا'])\n",
      "([], ['تاریخچه', 'هنر', 'نمایشی', 'در', 'ایران'])\n",
      "(['باشگاه فوتبال'], ['', 'اروپایی'])\n",
      "([], ['تاریخ', 'علوم', 'اجتماعی', 'در', 'اروپا'])\n",
      "([], ['جاذبه', 'گردشگری', 'در', 'استان', 'کردستان'])\n",
      "([], ['درمان', 'بیماری', 'افسردگی'])\n"
     ]
    }
   ],
   "source": [
    "def get_all_queries(directory):\n",
    "    queries = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open (os.path.join(directory, filename), 'r') as f:\n",
    "            query = f.readline()\n",
    "            queries.append(query)\n",
    "    return queries\n",
    "queries = get_all_queries('project1_data/data/queries/')\n",
    "for query in queries:\n",
    "    print(parsing_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect(phrase):\n",
    "    phrases = prepare_text(phrase)\n",
    "    if len(phrases) == 1:\n",
    "        return get_posting_list(phrases[0])\n",
    "    \n",
    "    phrase_touples = []\n",
    "    \n",
    "    for i in range(len(phrases) - 1):\n",
    "        phrase_touples.append((phrases[i], phrases[i + 1]))\n",
    "    \n",
    "    answer = {}\n",
    "    \n",
    "    for index, (phrase1, phrase2) in enumerate(phrase_touples):\n",
    "        doc_list1 = {}\n",
    "        if index == 0:\n",
    "            doc_list1 = positional_index[phrase1]\n",
    "            \n",
    "        elif not answer:\n",
    "            return answer\n",
    "        \n",
    "        else:\n",
    "            doc_list1 = answer\n",
    "            answer = {}\n",
    "\n",
    "        try:\n",
    "            doc_list_len1 = len(doc_list1)\n",
    "            doc_list2 = positional_index[phrase2]\n",
    "            doc_list_len2 = len(doc_list2)\n",
    "\n",
    "        except:\n",
    "            return answer\n",
    "        i, j = 0, 0\n",
    "        while i < doc_list_len1 and j < doc_list_len2:\n",
    "            doc1_keys = list(doc_list1.keys())\n",
    "            doc2_keys = list(doc_list2.keys())\n",
    "\n",
    "\n",
    "            doc1_key = doc1_keys[i]\n",
    "            doc2_key = doc2_keys[j]\n",
    "\n",
    "\n",
    "            if doc1_key == doc2_key:\n",
    "                doc_id = doc1_key\n",
    "                contexts = ['title', 'text']\n",
    "                for context in contexts:\n",
    "                    try:\n",
    "                        context_indices1 = doc_list1[doc1_key][context]\n",
    "                        context_indices2 = doc_list2[doc2_key][context]\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    m, n = 0, 0\n",
    "                    while m < len(context_indices1) and n < len(context_indices2):\n",
    "                        first_phrase_index = context_indices1[m]\n",
    "                        second_phrase_index = context_indices2[n]\n",
    "\n",
    "                        if second_phrase_index - first_phrase_index == 1:\n",
    "                            if doc_id not in answer:\n",
    "                                answer[doc_id] = {}\n",
    "\n",
    "                            if context not in answer[doc_id]:\n",
    "                                answer[doc_id][context] = []\n",
    "\n",
    "                            answer[doc_id][context].append(second_phrase_index)\n",
    "\n",
    "                            m += 1\n",
    "                            n += 1\n",
    "                        elif first_phrase_index < second_phrase_index:\n",
    "                            m += 1\n",
    "                        else:\n",
    "                            n += 1\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "\n",
    "            elif doc1_key < doc2_key:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "                \n",
    "#     print(answer)\n",
    "    return answer\n",
    "\n",
    "def get_phrase_occurence(phrase): \n",
    "    #count the number of occurence of phrase in docuemnts\n",
    "    documents = positional_intersect(phrase[:])\n",
    "    document_occurence = len(list(documents.keys()))\n",
    "    return document_occurence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_normalizer(double_quoted):\n",
    "    new_double_quotes = []\n",
    "    for quote in double_quoted:\n",
    "        quote_list = prepare_text(quote)\n",
    "        phrase = \"\"\n",
    "        for quotes in quote_list:\n",
    "            phrase += quotes + \" \"\n",
    "        new_double_quotes.append(phrase.strip())\n",
    "    return new_double_quotes\n",
    "\n",
    "def parsing_query(query):\n",
    "    all_quotations = re.findall('\"([^\"]*)\"', query)\n",
    "    query = query.replace('\"', \"\")\n",
    "    for quoted in all_quotations:\n",
    "        query = query.replace(quoted, \"\")\n",
    "        query = query.replace('  ', \" \")\n",
    "    normalized_phrases = phrase_normalizer(all_quotations[:])\n",
    "    not_sequential = prepare_text(query)\n",
    "    return normalized_phrases, not_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query مطالعه \"علوم اجتماعی\" در دانشگاه\n",
      "['title', 'text']\n",
      "terms ['علو اجتماع', 'مطالعه', 'در', 'دانشگاه']\n",
      "term: علو اجتماع query occ: 1 doc occ: 20\n",
      "term: مطالعه query occ: 1 doc occ: 96\n",
      "term: در query occ: 1 doc occ: 1240\n",
      "term: دانشگاه query occ: 1 doc occ: 322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'3069': 0.2557408645178948,\n",
       " '4066': 0.32556941983460935,\n",
       " '4388': 0.6677838781807702,\n",
       " '4497': 0.3239281498413287,\n",
       " '4568': 0.16128208931032467,\n",
       " '4833': 0.479255573197921,\n",
       " '4899': 0.20070264454360964,\n",
       " '5201': 0.2814089674058235,\n",
       " '5428': 0.804097782849659,\n",
       " '5449': 0.29239184936892626,\n",
       " '5571': 0.4333875765436848,\n",
       " '5872': 0.46785709709498713,\n",
       " '6506': 0.4939036577557021,\n",
       " '6589': 0.2958508036357524,\n",
       " '6634': 0.6258653344622294,\n",
       " '6694': 0.6698670241379651,\n",
       " '6791': 1.0164452807102937,\n",
       " '6907': 0.5722512968242297,\n",
       " '6973': 0.37791099742817885,\n",
       " '7002': 0.5168286121574529}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(doc_length)\n",
    "\n",
    "def get_quoted_posting_list(phrases):\n",
    "    first_double_quote = phrases.pop()\n",
    "    first_double_quote_info = positional_intersect(first_double_quote)\n",
    "    first_double_quote_docs = list(first_double_quote_info.keys()) #docs containing the first double quote\n",
    "    answer = first_double_quote_docs\n",
    "    \n",
    "    for double_quote in phrases:\n",
    "        double_quote_info = positional_intersect(double_quote)\n",
    "        if double_quote_info:            \n",
    "            other_double_quote_docs = list(double_quote_info.keys())\n",
    "            new_answer = []\n",
    "            for ans_doc_id in answer:\n",
    "                for doc_id_other in other_double_quote_docs:\n",
    "                    if ans_doc_id == doc_id_other:\n",
    "                        new_answer.append(ans_doc_id)\n",
    "            answer = new_answer\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "def get_dictionary(query_list):\n",
    "    dic = {}\n",
    "    for term in query_list:\n",
    "        if term not in dic:\n",
    "            dic[term] = 1\n",
    "        else:\n",
    "            dic[term] += 1\n",
    "    return dic\n",
    "\n",
    "\n",
    "def consine_score(query_list, method, weight, quoted=False, double_quoted=None, contexts=['title', 'text']):\n",
    "    if quoted: #new\n",
    "        valid_docs = get_quoted_posting_list(double_quoted[:]) #docs that have all phrases\n",
    "        \n",
    "    else:\n",
    "        valid_docs = list(doc_length.keys())\n",
    "        \n",
    "    scores = {} #score of documents\n",
    "    query_terms_occurence = get_dictionary(query_list) #terms occurence in query\n",
    "    terms = list(query_terms_occurence.keys()) #all distinct terms in query\n",
    "    doc_normalization = 0\n",
    "    \n",
    "    print('terms', terms)\n",
    "\n",
    "    for term in terms:\n",
    "        if term in positional_index or term in double_quoted:\n",
    "            if quoted:\n",
    "                docs_list = positional_intersect(term)\n",
    "            else:\n",
    "                docs_list = positional_index[term]\n",
    "\n",
    "            tf_q = query_terms_occurence[term]\n",
    "            document_occurence = len(docs_list)\n",
    "            print('term:', term, 'query occ:', query_terms_occurence[term], 'doc occ:', document_occurence)\n",
    "            \n",
    "            idf = math.log(N / document_occurence)\n",
    "            w_tq = tf_q * idf\n",
    "\n",
    "            for doc in docs_list:\n",
    "                if doc in valid_docs:\n",
    "                    tf = 0\n",
    "                    for context in contexts:\n",
    "                        if 'title' in docs_list[doc] and context == 'title':\n",
    "                            title_length = len(docs_list[doc]['title'])\n",
    "                            tf += weight * (1 + math.log(title_length))\n",
    "                        elif 'text' in docs_list[doc] and context == 'text':\n",
    "                            text_length = len(docs_list[doc]['text'])\n",
    "                            tf += 1 + math.log(text_length)    \n",
    "                        w_td = tf\n",
    "                        doc_normalization += w_td * w_td\n",
    "                        if doc in scores:\n",
    "                            scores[doc] += w_tq * w_td\n",
    "                        else:\n",
    "                            scores[doc] = w_tq * w_td\n",
    "    if method == 'ltc-lnc':\n",
    "        doc_normalization = math.sqrt(doc_normalization)\n",
    "        for doc in scores:\n",
    "            scores[doc] /= doc_normalization\n",
    "            \n",
    "    scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)} #TODO: not works\n",
    "    return scores\n",
    "\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2, context=['title', 'text']):\n",
    "    print('query', query)\n",
    "    print(context[:])\n",
    "    double_quoted, not_sequential = parsing_query(query)\n",
    "\n",
    "    all_words = [] #new\n",
    "    all_words.extend(double_quoted + not_sequential) #new\n",
    "\n",
    "    if not double_quoted:\n",
    "        return consine_score(all_words[:], method, weight)\n",
    "    \n",
    "    return consine_score(all_words, method, weight, True, double_quoted[:])\n",
    "\n",
    "search(queries[1], 'ltc-lnc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query عجایب هفت‌گانه\n",
      "['title']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "consine_score() got an unexpected keyword argument 'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-feb35ebe32d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrelevant_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdetailed_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'عجایب هفت‌گانه'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'چشمگیرترین بناهای تاریخی جهان'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ltc-lnc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-291-feb35ebe32d5>\u001b[0m in \u001b[0;36mdetailed_search\u001b[0;34m(title_query, text_query, method)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetailed_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ltn-lnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtitle_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(title_query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(desc_query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-290-6508124dc591>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(query, method, weight, context)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdouble_quoted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconsine_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconsine_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_quoted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: consine_score() got an unexpected keyword argument 'context'"
     ]
    }
   ],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    relevant_docs = {}\n",
    "    title_score = search(title_query, method=method, weight=2,context=['title'])\n",
    "    #print(title_query)\n",
    "    #print(desc_query)\n",
    "    return relevant_docs\n",
    "\n",
    "detailed_search('عجایب هفت‌گانه', 'چشمگیرترین بناهای تاریخی جهان', \"ltc-lnc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

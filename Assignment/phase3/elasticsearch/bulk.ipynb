{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T20:29:54.735281Z",
     "start_time": "2020-06-22T20:29:54.631118Z"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(\"localhost:9200\")\n",
    "\n",
    "def index_data():\n",
    "    indexed_docs = list()\n",
    "    with open('./data.json', 'r') as json_file:\n",
    "        docs = json.load(json_file)\n",
    "    \n",
    "    for index, doc in enumerate(docs[:1]):\n",
    "        doc_str = json.dumps(doc)\n",
    "#         print(doc_str)\n",
    "        es.index(index='paper', id=index, body=doc)\n",
    "\n",
    "        \n",
    "index_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T23:07:35.390045Z",
     "start_time": "2020-06-22T23:07:35.341870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 1, 'total': 1},\n",
       " 'hits': {'hits': [{'_id': '1110',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'For machine reading comprehension, the capacity of effectively modeling the linguistic knowledge from the detail-riddled and lengthy passages and getting ride of the noises is essential to improve its performance. Traditional attentive models attend to all words without explicit constraint, which results in inaccurate concentration on some dispensable words. In this work, we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanism for better linguistically motivated word representations. In detail, for self-attention network (SAN) sponsored Transformer-based encoder, we introduce syntactic dependency of interest (SDOI) design into the SAN to form an SDOI-SAN with syntax-guided self-attention. Syntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation. To verify its effectiveness, the proposed SG-Net is applied to typical pre-trained language model BERT which is right based on a Transformer encoder. Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE show that the proposed SG-Net design helps achieve substantial performance improvement over strong baselines.',\n",
       "      'authors': ['Zhuosheng Zhang', 'Yu-Wei Wu', 'Rui Wang'],\n",
       "      'date': '2020',\n",
       "      'id': '67b1f8e48118bb1aa250f400d475425317bf4117',\n",
       "      'references': ['/paper/Semantics-aware-BERT-for-Language-Understanding-Zhang-Wu/fbace1600c2f58a2f39640c49fd7dab5fd4a1e49',\n",
       "       '/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af',\n",
       "       '/paper/LIMIT-BERT-%3A-Linguistic-Informed-Multi-Task-BERT-Zhou-Zhang/07588dd5d0252c7abc99b3834a81bf23741ead4b',\n",
       "       '/paper/Neural-Module-Networks-for-Reasoning-over-Text-Gupta-Lin/4cdf436857c23c5f7d1aad51e2a8124faf0070d2',\n",
       "       '/paper/Retrospective-Reader-for-Machine-Reading-Zhang-Yang/8d00049c345b9c8cc76ea2ea2565f8bb69f6b683',\n",
       "       '/paper/Neural-Machine-Translation-with-Universal-Visual-Zhang-Chen/834e987e107e251441970a6e5058446f1dd8a005',\n",
       "       '/paper/Probing-Contextualized-Sentence-Representations-Zhang-Wang/5bca8ca79077db1b49bcb4869c98e77ad40878bd',\n",
       "       '/paper/RikiNet%3A-Reading-Wikipedia-Pages-for-Natural-Liu-Gong/a93e6c7762125c465c467d4b07b2872369db4ce5',\n",
       "       '/paper/Frustratingly-Easy-Natural-Question-Answering-Pan-Chakravarti/c7e04335452e988e2be5f1d132e7f6eadad13fd3',\n",
       "       '/paper/An-Introduction-of-Deep-Learning-Based-Word-Applied-Fu/c82d20f93eecc764a41fca89c2f78c758fe91dc4',\n",
       "       '/paper/Linguistically-Informed-Self-Attention-for-Semantic-Strubell-Verga/060ff1aad5619a7d6d6cdfaf8be5da29bff3808c',\n",
       "       '/paper/Improving-Machine-Reading-Comprehension-with-Sun-Yu/8f346de21a13dacf5b65e2de81e84d54226a5b9f',\n",
       "       '/paper/Semantics-aware-BERT-for-Language-Understanding-Zhang-Wu/fbace1600c2f58a2f39640c49fd7dab5fd4a1e49',\n",
       "       '/paper/Subword-augmented-Embedding-for-Cloze-Reading-Zhang-Huang/2775fec343d569c92b4fe15d3a2a2b9b331eff13',\n",
       "       '/paper/Syntax-aware-Transformer-Encoder-for-Neural-Machine-Duan-Zhao/b134ce39a3a195f5b0c277a1d2c0d776e5ce6a52',\n",
       "       '/paper/Syntax-Directed-Attention-for-Neural-Machine-Chen-Wang/4ddd5d6c973632f977ff3a92c3233e41f097b096',\n",
       "       '/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "       '/paper/Graph-Convolutional-Encoders-for-Syntax-aware-Bastings-Titov/2784000e1a3554374662f4d18cb5ad52f59c8de6',\n",
       "       '/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050',\n",
       "       '/paper/Multi-Granularity-Hierarchical-Attention-Fusion-for-Wang-Wu/26b47e35fe6e4260fdf7b7cc98f279a73c277494'],\n",
       "      'title': 'SG-Net: Syntax-Guided Machine Reading Comprehension'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1111',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.',\n",
       "      'authors': ['Seohyun Back', 'Sai Chetan Chinthakindi', 'Jaegul Choo'],\n",
       "      'date': '2020',\n",
       "      'id': 'f402e5990b2d95587a0c733f7c1ec19ee5c81781',\n",
       "      'references': ['/paper/Retrospective-Reader-for-Machine-Reading-Zhang-Yang/8d00049c345b9c8cc76ea2ea2565f8bb69f6b683',\n",
       "       '/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af',\n",
       "       '/paper/Machine-Reading-Comprehension-with-Abstention-Finkelstein-Rawat/668c0e534bac868237a4d33ee75bc243afc7e06c',\n",
       "       '/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88',\n",
       "       '/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050',\n",
       "       \"/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c\",\n",
       "       '/paper/HotpotQA%3A-A-Dataset-for-Diverse%2C-Explainable-Yang-Qi/22655979df781d222eaf812b0d325fa9adf11594',\n",
       "       '/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5',\n",
       "       '/paper/Gated-Self-Matching-Networks-for-Reading-and-Wang-Yang/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f',\n",
       "       '/paper/MS-MARCO%3A-A-Human-Generated-MAchine-Reading-Dataset-Campos-Nguyen/a69cf45d44a9d806d2487a1ffb9eca71ee73c2ee',\n",
       "       '/paper/Answering-while-Summarizing%3A-Multi-task-Learning-QA-Nishida-Nishida/bed4621f62e3955b93285047a99013106998498c',\n",
       "       '/paper/NewsQA%3A-A-Machine-Comprehension-Dataset-Trischler-Wang/3eda43078ae1f4741f09be08c4ecab6229046a5c',\n",
       "       '/paper/Simple-and-Effective-Multi-Paragraph-Reading-Clark-Gardner/3c78c6df5eb1695b6a399e346dde880af27d1016'],\n",
       "      'title': 'NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1112',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Existing machine reading comprehension models are reported to be brittle for adversarially perturbed questions when optimizing only for accuracy, which led to the creation of new reading comprehension benchmarks, such as SQuAD 2.0 which contains such type of questions. However, despite the super-human accuracy of existing models on such datasets, it is still unclear how the model predicts the answerability of the question, potentially due to the absence of a shared annotation for the explanation. To address such absence, we release SQuAD2-CR dataset, which contains annotations on unanswerable questions from the SQuAD 2.0 dataset, to enable an explanatory analysis of the model prediction. Specifically, we annotate (1) explanation on why the most plausible answer span cannot be the answer and (2) which part of the question causes unanswerability. We share intuitions and experimental results that how this dataset can be used to analyze and improve the interpretability of existing reading comprehension model behavior.',\n",
       "      'authors': ['Gyeongbok Lee', 'Seung-won Hwang', 'Hyunsouk Cho'],\n",
       "      'date': '2020',\n",
       "      'id': '27faa3ad168a64ad46a2aecfdb8b302d8217d811',\n",
       "      'references': [\"/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c\",\n",
       "       '/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88',\n",
       "       '/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050',\n",
       "       '/paper/Learning-to-Ask-Unanswerable-Questions-for-Machine-Zhu-Dong/02dbc43fb947eda8f3b83bc085b4deb0f07010f5',\n",
       "       '/paper/AllenNLP-Interpret%3A-A-Framework-for-Explaining-of-Wallace-Tuyls/ddd27dba038d0ed14c48cd027812df58a902ece2',\n",
       "       '/paper/QADiver%3A-Interactive-Framework-for-Diagnosing-QA-Lee-Kim/1853d80be81551084a3f4b3cc873a33ad9b0a6d5',\n",
       "       '/paper/A-Qualitative-Comparison-of-CoQA%2C-SQuAD-2.0-and-Yatskar/0a5606f0d56c618aa610cb1677e2788a3bd678fa',\n",
       "       '/paper/Adversarial-Examples-for-Evaluating-Reading-Systems-Jia-Liang/ffb949d3493c3b2f3c9acf9c75cb03938933ddf0',\n",
       "       '/paper/Zero-Shot-Relation-Extraction-via-Reading-Levy-Seo/fa025e5d117929361bcf798437957762eb5bb6d4',\n",
       "       '/paper/Question-Answering-with-Grammatically-Interpretable-Palangi-Smolensky/d4a9876f00fd9caf0358fd73e5572e53a47cda12'],\n",
       "      'title': 'SQuAD2-CR: Semi-supervised Annotation for Cause and Rationales for Unanswerability in SQuAD 2.0'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1113',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'A hashtag is a type of metadata tag used on social networks and can help people search for specific topics or content. To capture the interactive information between words and understand the content of microblog posts deeply, this study proposed a neural network model based on a word-level self-attention mechanism. Given a microblog post, the weight of each word was calculated through a self-attention mechanism, and then the representation of a microblog post was obtained through the weighted summation of words. Finally, a multi-layer perceptron was adopted on the representation of a microblog post to perform the classification. The effectiveness of the proposed model was verified through experiments of large-scale datasets. Results show that: (1) introducing word-level self-attention mechanism into hashtag recommendation is effective. (2) In comparison with the baseline methods used in previous studies, such as convolutional neural network or long short-term memory network, the proposed self-attentive neural networks can provide a more accurate representation of a microblog post and significantly improve the F-score of hashtag recommendation on the same dataset. This study provides references for the methods and evaluation of short-text hashtag recommendations, such as microblogs.',\n",
       "      'authors': ['Delu Yang', 'Rui Zhu', 'Yao Li'],\n",
       "      'date': '2019',\n",
       "      'id': 'b22c90204d0dd150e88b0ebdbbac98172647693b',\n",
       "      'references': ['/paper/Microblogging-Hashtag-Recommendation-Considering-Anandhan-Shuib/50dc177aa2d0537c9c4bf113e2ca0dfb99feaf87',\n",
       "       '/paper/Topical-Co-Attention-Networks-for-hashtag-on-Li-Liu/5e78b6f9a45c364cca82c232cb5b899b969cfd55',\n",
       "       '/paper/Learning-Improved-Semantic-Representations-with-for-Zhu-Yang/0a62b02c6f7e03e6345d43989a40c351f65cbc74',\n",
       "       '/paper/Hashtag-Recommendation-Using-Attention-Based-Neural-Gong-Zhang/267efba3e61afc3010021a44b93da3e44170a737',\n",
       "       '/paper/Hashtag-Recommendation-with-Topical-Attention-Based-Li-Liu/0087d3ee3e66498483dbc9e799f2744f562a75b7',\n",
       "       '/paper/Hashtag-Recommendation-for-Multimodal-Microblog-Zhang-Wang/d00f6ec074bbe777ba2e419b39729283a28101c5',\n",
       "       '/paper/Hashtag-recommendation-for-multimodal-microblog-Gong-Zhang/02d60d1a9750110467f94c9ee771846883c8e06b',\n",
       "       '/paper/Temporal-Effects-on-Hashtag-Reuse-in-Twitter%3A-A-Kowald-Pujari/5a0c1bd410b59cb8b7f8cfb4a9146c651d40a76e',\n",
       "       '/paper/Automatic-Hashtag-Recommendation-for-Microblogs-Ding-Zhang/c014069630c39dd42a31bea37c7a592a8f3ca66a',\n",
       "       '/paper/What-to-Tag-Your-Microblog%3A-Hashtag-Recommendation-Wang-Qu/6ca529af63a29b1122d8007c3784a9612c793e1a',\n",
       "       '/paper/Using-topic-models-for-Twitter-hashtag-Godin-Slavkovikj/e266cd53a6224f934b8e05be5895b8f898aa7e9e'],\n",
       "      'title': 'Self-Attentive Neural Network for Hashtag Recommendation'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1114',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'This study tackles generative reading comprehension (RC), which consists of answering questions based on textual evidence and natural language generation (NLG). We propose a multi-style abstractive summarization model for question answering, called Masque. The proposed model has two key characteristics. First, unlike most studies on RC that have focused on extracting an answer span from the provided passages, our model instead focuses on generating a summary from the question and multiple passages. This serves to cover various answer styles required for real-world applications. Second, whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model, our approach learns multi-style answers within a model to improve the NLG capability for all styles involved. This also enables our model to give an answer in the target style. Experiments show that our model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the transfer of the style-independent NLG capability to the target style is the key to its success.',\n",
       "      'authors': ['Kyosuke Nishida', 'Itsumi Saito', 'Junji Tomita'],\n",
       "      'date': '2019',\n",
       "      'id': '4c09d30704c0ceb128bb31ee09f957ee58d5032c',\n",
       "      'references': ['/paper/Multiple-Perspective-Answer-Reranking-for-Reading-Ren-Huang/23c06690e5c85e034ba53e6fad739a37e4a5fe2f',\n",
       "       '/paper/A-Study-of-the-Tasks-and-Models-in-Machine-Reading-Wang/0f7a4adce630c823ab00620e6c30591d7cdc1903',\n",
       "       '/paper/Machine-Reading-Comprehension%3A-The-Role-of-Language-Zhang-Zhao/aa9c6d43b36a55b34c2e9207355d355fd94691af',\n",
       "       '/paper/BookQA%3A-Stories-of-Challenges-and-Opportunities-Angelidis-Frermann/aa2ba55f349ee5eff02497057c53eae29be67a74',\n",
       "       '/paper/Joint-Learning-of-Answer-Selection-and-Answer-in-Deng-Lam/65b5fe92bec0941988d7fa32a24bbcad63e53866',\n",
       "       '/paper/A-Discrete-Hard-EM-Approach-for-Weakly-Supervised-Min-Chen/30eff53e981695c7296d258b8dc44b4c7b482a0c',\n",
       "       '/paper/Generating-a-Common-Question-from-Multiple-using-Cho-Zhang/ba06b29ff25ce21a95243518f1ebf87c5e82493a',\n",
       "       '/paper/Contextualized-Embeddings-based-Transformer-Encoder-Laskar-Huang/28307bc149a74cfcae657f782f1c7630b6f4acce',\n",
       "       '/paper/Probabilistic-Assumptions-Matter%3A-Improved-Models-Cheng-Chang/74abf8638a3dde78f20047dc72413780f2c28fb7',\n",
       "       '/paper/Short-Text-Conversation-Based-on-Deep-Neural-and-on-Cherng-Chang/c9847199c36a4b284cfbb0f18d4140d0b11d8e7e',\n",
       "       '/paper/Commonsense-for-Generative-Multi-Hop-Question-Tasks-Bauer-Wang/711b1f7cc4e92d6f40c7813c6f0e1c2e179d48ad',\n",
       "       '/paper/S-Net%3A-From-Answer-Extraction-to-Answer-Synthesis-Tan-Wei/49f4ab44c9672f88637b7feaa182cf0b8b07a4c8',\n",
       "       '/paper/A-Deep-Cascade-Model-for-Multi-Document-Reading-Yan-Xia/c435d84e38575b0ba52420bbc62230db8cd5bc14',\n",
       "       '/paper/Simple-and-Effective-Multi-Paragraph-Reading-Clark-Gardner/3c78c6df5eb1695b6a399e346dde880af27d1016',\n",
       "       '/paper/Multi-Passage-Machine-Reading-Comprehension-with-Wang-Liu/0985497d1de3ffd11713e75289cc2ad55836623d',\n",
       "       '/paper/DuReader%3A-a-Chinese-Machine-Reading-Comprehension-He-Liu/995b7affd684b910d5a1c520c3af00fd20cc39b0',\n",
       "       '/paper/DuoRC%3A-Towards-Complex-Language-Understanding-with-Saha-Aralikatte/a7bbb084f5de4f318c811776afeba2b05439c234',\n",
       "       '/paper/Retrieve-and-Read%3A-Multi-task-Learning-of-Retrieval-Nishida-Saito/4b69fbc425daf8f60f4dce42696b9b6d08d6c64f',\n",
       "       '/paper/The-NarrativeQA-Reading-Comprehension-Challenge-Kocisk%C3%BD-Schwarz/d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a',\n",
       "       '/paper/Gated-Self-Matching-Networks-for-Reading-and-Wang-Yang/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f'],\n",
       "      'title': 'Multi-style Generative Reading Comprehension'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1115',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Machine Reading Comprehension (MRC) is an important topic in the domain of automated question answering and in natural language processing more generally. Since the release of the SQuAD 1.1 and SQuAD 2 datasets, progress in the field has been particularly significant, with current state-of-the-art models now exhibiting near-human performance at both answering well-posed questions and detecting questions which are unanswerable given a corresponding context. In this work, we present Enhanced Question Answer Network (EQuANt), an MRC model which extends the successful QANet architecture of Yu et al. to cope with unanswerable questions. By training and evaluating EQuANt on SQuAD 2, we show that it is indeed possible to extend QANet to the unanswerable domain. We achieve results which are close to 2 times better than our chosen baseline obtained by evaluating a lightweight version of the original QANet architecture on SQuAD 2. In addition, we report that the performance of EQuANt on SQuAD 1.1 after being trained on SQuAD2 exceeds that of our lightweight QANet architecture trained and evaluated on SQuAD 1.1, demonstrating the utility of multi-task learning in the MRC context.',\n",
       "      'authors': ['Franccois-Xavier Aubet', 'D. Danks', 'Yuchen Zhu'],\n",
       "      'date': '2019',\n",
       "      'id': 'abfba26234560c097145537f20c1c57615f33272',\n",
       "      'references': [\"/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c\",\n",
       "       '/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050',\n",
       "       '/paper/Read-%2B-Verify%3A-Machine-Reading-Comprehension-with-Hu-Wei/9a5ba9aee44ab873f3d60b05e2773c693707da88',\n",
       "       '/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5',\n",
       "       '/paper/Stochastic-Answer-Networks-for-SQuAD-2.0-Liu-Li/d0095adcaa33bc549e273a824c3b66d92897fad8',\n",
       "       '/paper/QANet%3A-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101',\n",
       "       '/paper/Gated-Self-Matching-Networks-for-Reading-and-Wang-Yang/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f',\n",
       "       '/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4',\n",
       "       '/paper/Zero-Shot-Relation-Extraction-via-Reading-Levy-Seo/fa025e5d117929361bcf798437957762eb5bb6d4',\n",
       "       '/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992'],\n",
       "      'title': 'EQuANt (Enhanced Question Answer Network)'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1116',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Machine reading comprehension combined with unanswerable questions is a novel, challenging task in the field of natural language processing, which motivated the creation of the SQuAD 2.0 dataset. We experiment with implementing a hierarchical attention fusion network to capture complex relationships between the context and question. We also adopt a multi-task learning approach to the problem by adding an answer verifier to the output of our model which predicts whether or not a question is answerable, and further experiment with plausible answer pointers as well. We built the architecture from scratch, replacing components of the generously provided starter code framework of a BiDAF model. After model tuning, we obtain an F1 score of 65.48 and an EM score of 63.06 on the held-out test set, which ranks competitively on the class leaderboard. In our work, we use ablative analysis to show the benefits of each component of our network, as well as qualitative analysis of model predictions.',\n",
       "      'authors': ['Woody Wang'],\n",
       "      'date': '2019',\n",
       "      'id': '19fa2456db077bafc5f04a87126e84d0f39b5f65',\n",
       "      'references': ['/paper/Multi-Granularity-Hierarchical-Attention-Fusion-for-Wang-Wu/26b47e35fe6e4260fdf7b7cc98f279a73c277494',\n",
       "       '/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050',\n",
       "       '/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4',\n",
       "       '/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "       '/paper/Reading-Wikipedia-to-Answer-Open-Domain-Questions-Chen-Fisch/104715e1097b7ebee436058bfd9f45540f269845',\n",
       "       \"/paper/Know-What-You-Don't-Know%3A-Unanswerable-Questions-Rajpurkar-Jia/4d1c856275744c0284312a3a50efb6ca9dc4cd4c\",\n",
       "       '/paper/Zoneout%3A-Regularizing-RNNs-by-Randomly-Preserving-Krueger-Maharaj/9f0687bcd0a7d7fc91b8c5d36c003a38b8853105'],\n",
       "      'title': 'Question Answering with Hierarchical Attention Fusion Layers & MultiTask Learning'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1117',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view.',\n",
       "      'authors': ['Lukasz Maziarka', 'Tomasz Danel', 'Stanislaw Jastrzebski'],\n",
       "      'date': '2020',\n",
       "      'id': 'b671c5622e02f0c4e9dac24116a5a3c9db7b392e',\n",
       "      'references': ['/paper/GEOM%3A-Energy-annotated-molecular-conformations-for-Axelrod-G%C3%B3mez-Bombarelli/27bde53a14b2ee8bc60a67cb34b9ea5c3ba85603',\n",
       "       '/paper/Graph-Aware-Transformer%3A-Is-Attention-All-Graphs-Yoo-Kim/3d0b3b6239d79f4b0c121c299ee4a0d2137d3816',\n",
       "       '/paper/Enhanced-Deep-Learning-Prediction-of-Molecular-via-Cho-Choi/01ff91216a20e7081bf1ab5d7a405ce5b8b33a21',\n",
       "       '/paper/AtomNet%3A-A-Deep-Convolutional-Neural-Network-for-in-Wallach-Dzamba/f276c869b4b75360f6cba3b65dae91b5f4f322b7',\n",
       "       '/paper/PotentialNet-for-Molecular-Property-Prediction-Feinberg-Sur/8f7a7aea33c942e626832f014a831361b7fa39c7',\n",
       "       '/paper/SMILES-BERT%3A-Large-Scale-Unsupervised-Pre-Training-Wang-Guo/3d99747cc3e13d22f21e02c35e82b57d2e351e2a',\n",
       "       '/paper/Neural-Message-Passing-for-Quantum-Chemistry-Gilmer-Schoenholz/e24cdf73b3e7e590c2fe5ecac9ae8aa983801367',\n",
       "       '/paper/Convolutional-Embedding-of-Attributed-Molecular-for-Coley-Barzilay/5589a7a9e5a6b63305077508d6b2c281d7d095ed',\n",
       "       '/paper/Analyzing-Learned-Molecular-Representations-for-Yang-Swanson/573506b96ed380044f33b30b66de10e25959b0fd',\n",
       "       '/paper/Graph-Attention-Networks-Velickovic-Cucurull/33998aff64ce51df8dee45989cdca4b6b1329ec4',\n",
       "       '/paper/Chemception%3A-A-Deep-Neural-Network-with-Minimal-the-Goh-Siegel/efd4174ab99df6ee321779d68fd24dcb8fd5d23c',\n",
       "       '/paper/Gaussian-Transformer%3A-A-Lightweight-Approach-for-Guo-Zhang/84898960f68fa78296a102edc8ac81739f9a9408'],\n",
       "      'title': 'Molecule Attention Transformer'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1118',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.',\n",
       "      'authors': ['Baosong Yang', 'Longyue Wang', 'Zhaopeng Tu'],\n",
       "      'date': '2019',\n",
       "      'id': 'bdc046e65bc80cf13929ca0c3934d6faee830723',\n",
       "      'references': ['/paper/Cross-Aggregation-of-Multi-head-Attention-for-Cao-Hai/4d68c1b4167f858979c6a8e8b9ad0b484cd48c63',\n",
       "       '/paper/Leveraging-Local-and-Global-Patterns-for-Networks-Xu-Wong/2353f65b42e9f445425568088c5adef300a7f573',\n",
       "       '/paper/Low-Rank-and-Locality-Constrained-Self-Attention-Guo-Qiu/2a02c967dd9848064bca0aa69ea6c75b3765d0ee',\n",
       "       '/paper/Self-Attention-with-Structural-Position-Wang-Tu/4ca4e7ed290617aab8b4d99294d10b0ac8372967',\n",
       "       '/paper/Fixed-Encoder-Self-Attention-Patterns-in-Machine-Raganato-Scherrer/57f123c95ecf9d901be3a53291f53302740451e2',\n",
       "       '/paper/SesameBERT%3A-Attention-for-Anywhere-Su-Cheng/553c1048e90e84575cad9016f367cf69c52a7fd7',\n",
       "       '/paper/Two-Headed-Monster-And-Crossed-Co-Attention-Li-Jiang/863bc093e87a2963a0f6b5fc6c2d8b7451b8a7b2',\n",
       "       '/paper/How-Does-Selective-Mechanism-Improve-Self-Attention-Geng-Wang/812aed3f4032bd28a14d4cd3a40c77cb82066b9c',\n",
       "       '/paper/Capsule-Transformer-for-Neural-Machine-Translation-Duan-Cao/876be9b226601821eeade310013506a03f023824',\n",
       "       '/paper/Attention-Augmented-Convolutional-Networks-Bello-Zoph/27ac832ee83d8b5386917998a171a0257e2151e2',\n",
       "       '/paper/Context-Aware-Self-Attention-Networks-Yang-Li/5d1a6d27d67e0b8d4a049f7f5dc3995f837d7976',\n",
       "       '/paper/DiSAN%3A-Directional-Self-Attention-Network-for-Shen-Zhou/adc276e6eae7051a027a4c269fb21dae43cadfed',\n",
       "       '/paper/Modeling-Recurrence-for-Transformer-Hao-Wang/68a47a65c1ffd5d1540f12adde1a7300594c9969',\n",
       "       '/paper/Gaussian-Transformer%3A-A-Lightweight-Approach-for-Guo-Zhang/84898960f68fa78296a102edc8ac81739f9a9408',\n",
       "       '/paper/Modeling-Localness-for-Self-Attention-Networks-Yang-Tu/1af138dc72fa855cc3bc9c0b83750b461c26e29d',\n",
       "       '/paper/Self-Attention-with-Relative-Position-Shaw-Uszkoreit/c8efcc854d97dfc2a42b83316a2109f9d166e43f',\n",
       "       '/paper/QANet%3A-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101',\n",
       "       '/paper/Multi-Head-Attention-with-Disagreement-Li-Tu/fdbdd4e0461d23905104460a02a176907d945f44',\n",
       "       '/paper/An-Analysis-of-Encoder-Representations-in-Machine-Raganato-Tiedemann/94238dead40b12735d79ed63e29ead70730261a2',\n",
       "       '/paper/Bi-Directional-Block-Self-Attention-for-Fast-and-Shen-Zhou/0ef460c47377c3b9482d8177cbcafad1730a91a5'],\n",
       "      'title': 'Convolutional Self-Attention Networks'}},\n",
       "    '_type': '_doc'},\n",
       "   {'_id': '1119',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at this https URL.',\n",
       "      'authors': ['Xinwei Geng', 'Longyue Wang', 'Zhaopeng Tu'],\n",
       "      'date': '2020',\n",
       "      'id': '812aed3f4032bd28a14d4cd3a40c77cb82066b9c',\n",
       "      'references': ['/paper/Assessing-the-Ability-of-Self-Attention-Networks-to-Yang-Wang/f89d2da991935549b109d780be3351e0dda92a8f',\n",
       "       '/paper/Why-Self-Attention-A-Targeted-Evaluation-of-Neural-Tang-M%C3%BCller/e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1',\n",
       "       '/paper/Self-Attention-with-Structural-Position-Wang-Tu/4ca4e7ed290617aab8b4d99294d10b0ac8372967',\n",
       "       '/paper/Linguistically-Informed-Self-Attention-for-Semantic-Strubell-Verga/060ff1aad5619a7d6d6cdfaf8be5da29bff3808c',\n",
       "       '/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0',\n",
       "       '/paper/Convolutional-Self-Attention-Networks-Yang-Wang/bdc046e65bc80cf13929ca0c3934d6faee830723',\n",
       "       '/paper/Phrase-level-Self-Attention-Networks-for-Universal-Wu-Wang/cd0bb48f0f26f146759654bdc2dd0aad87e917de',\n",
       "       '/paper/Towards-Better-Modeling-Hierarchical-Structure-for-Hao-Wang/a940e88117d19160f276e9b6ae014df699495a3f',\n",
       "       '/paper/Extracting-Syntactic-Trees-from-Transformer-Encoder-Marecek-Rosa/49400b3a3ea01772e321e3e010b7b891c3d6cb88',\n",
       "       '/paper/Deep-Semantic-Role-Labeling-with-Self-Attention-Tan-Wang/6ed376a26045ff0048ec2b216785d396960d6ed1'],\n",
       "      'title': 'How Does Selective Mechanism Improve Self-Attention Networks?'}},\n",
       "    '_type': '_doc'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': {'relation': 'eq', 'value': 1996}},\n",
       " 'timed_out': False,\n",
       " 'took': 4}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index='paper', body={\"query\": {\n",
    "            \"match_all\": {}\n",
    "        }})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

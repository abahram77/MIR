{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T20:29:54.735281Z",
     "start_time": "2020-06-22T20:29:54.631118Z"
    }
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "\n",
    "es = Elasticsearch(\"localhost:9200\")\n",
    "\n",
    "def index_data():\n",
    "    indexed_docs = list()\n",
    "    with open('./data.json', 'r') as json_file:\n",
    "        docs = json.load(json_file)\n",
    "    \n",
    "    for index, doc in enumerate(docs[:1]):\n",
    "        doc_str = json.dumps(doc)\n",
    "#         print(doc_str)\n",
    "        es.index(index='paper', id=index, body=doc)\n",
    "\n",
    "        \n",
    "index_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T20:30:52.502687Z",
     "start_time": "2020-06-22T20:30:51.312452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'failed': 0, 'skipped': 0, 'successful': 1, 'total': 1},\n",
       " 'hits': {'hits': [{'_id': '0',\n",
       "    '_index': 'paper',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'paper': {'abstract': 'Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. \\nThis paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. \\nThe lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.',\n",
       "      'authors': ['Jonathan Frankle', 'Michael Carbin'],\n",
       "      'date': '2018',\n",
       "      'id': 'f90720ed12e045ac84beb94c27271d6fb8ad48cf',\n",
       "      'references': ['/paper/The-Lottery-Ticket-Hypothesis-at-Scale-Frankle-Dziugaite/03e0cbeb4604262446a97cb381874c7de1cffea2',\n",
       "       '/paper/Successfully-Applying-the-Stabilized-Lottery-Ticket-Brix-Bahar/27a04669fef49981b91a6a148e549360ad309761',\n",
       "       '/paper/Luck-Matters%3A-Understanding-Training-Dynamics-of-Tian-Jiang/308e0a4523537e9fdc25a6afff67f9d214738d76',\n",
       "       '/paper/Rethinking-the-Value-of-Network-Pruning-Liu-Sun/cdb25e4df6913bb94edcd1174d00baf2d21c9a6d',\n",
       "       '/paper/Neural-Rejuvenation%3A-Improving-Deep-Network-by-Qiao-Lin/3ba9fdcd0d10f2a130fcbd678cebcbb5e8c6bd5f',\n",
       "       '/paper/SiPPing-Neural-Networks%3A-Sensitivity-informed-of-Baykal-Liebenwein/65dca1da228dd3cedc4431d1caa6c30038516966',\n",
       "       '/paper/SuperNet-An-efficient-method-of-neural-networks-Bukowski-Dzwinel/dfa8e748c16995f88134e6df44b1a6817d2bd6cd',\n",
       "       '/paper/Weight-Agnostic-Neural-Networks-Gaier-Ha/23d7d2aae6308a840a597e823ae8214278304c5a',\n",
       "       '/paper/Learning-Sparse-Networks-Using-Targeted-Dropout-Gomez-Zhang/13de3c06ef6dac1c296ada45df2be590f843edb7',\n",
       "       '/paper/Targeted-Dropout-Talwar/ea19fee3ecce6f1b17dfde38bce1ccceaeb5befb',\n",
       "       '/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698',\n",
       "       '/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff',\n",
       "       '/paper/Data-free-Parameter-Pruning-for-Deep-Neural-Srinivas-Babu/b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d',\n",
       "       '/paper/Understanding-Dropout-Baldi-Sadowski/cc46229a7c47f485e090857cbab6e6bf68c09811',\n",
       "       '/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f',\n",
       "       '/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3',\n",
       "       '/paper/Diversity-Networks%3A-Neural-Network-Compression-Mariet-Sra/2dfef5635c8c44431ca3576081e6cfe6d65d4862',\n",
       "       '/paper/A-Deep-Neural-Network-Compression-Pipeline%3A-Huffman-Han-Mao/397de65a9a815ec39b3704a79341d687205bc80a',\n",
       "       '/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a',\n",
       "       '/paper/Optimal-Brain-Surgeon-and-general-network-pruning-Hassibi-Stork/e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724'],\n",
       "      'title': 'The Lottery Ticket Hypothesis: Training Pruned Neural Networks'}},\n",
       "    '_type': '_doc'}],\n",
       "  'max_score': 1.0,\n",
       "  'total': {'relation': 'eq', 'value': 1}},\n",
       " 'timed_out': False,\n",
       " 'took': 1178}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index='paper', body={\"query\": {\"match_all\": {}}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
